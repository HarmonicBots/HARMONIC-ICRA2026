<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>HARMONIC: A Content-Centric Cognitive Robotic Architecture</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.2/css/academicons.min.css">
    <style>
    /* Base styles with simplified color scheme */
    :root {
      --primary-color: #2c3e50;  /* Deep blue-gray */
      --secondary-color: #34495e; /* Darker blue-gray */
      --accent-color: #3498db;   /* Subtle blue accent */
      --text-color: #333333;
      --light-gray: #f5f5f5;
      --border-color: #e0e0e0;
      --link-color: #2980b9;
    }
    
    * {
      box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
      font-family: 'Inter', sans-serif;
      line-height: 1.6;
      color: var(--text-color);
      background-color: white;
        }

        .container {
      max-width: 1400px;
      margin: 0 auto;
      padding: 0 20px;
    }
    
    /* Header styles */
    .header {
            text-align: center;
      padding: 40px 0 20px;
      border-bottom: 1px solid var(--border-color);
        }

    .paper-title {
      font-size: 32px;
            font-weight: 700;
      margin-bottom: 20px;
      color: var(--primary-color);
            line-height: 1.2;
        }

    .authors {
      margin: 20px 0;
      font-size: 18px;
      line-height: 1.7;
    }
    
    .authors a {
      color: var(--link-color);
      text-decoration: none;
      transition: color 0.2s;
    }
    
    .authors a:hover {
      text-decoration: underline;
    }
    
    .affiliations {
      margin: 15px 0 20px;
      font-size: 16px;
      line-height: 1.5;
      color: var(--secondary-color);
    }
    
    .affiliations strong {
      color: var(--primary-color);
    }
    
    .sup {
      font-size: 65%;
      vertical-align: super;
      color: var(--secondary-color);
    }
    
    /* Citation section */
    .citation-container {
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
      margin: 20px 0;
    }
    
    .citation-button {
      display: inline-flex;
      align-items: center;
      background-color: white;
      color: var(--link-color);
      border: 1px solid var(--border-color);
      border-radius: 4px;
      padding: 8px 16px;
      font-size: 16px;
      cursor: pointer;
      transition: all 0.2s;
      text-decoration: none;
    }
    
    .citation-button:hover {
      background-color: var(--light-gray);
      border-color: var(--accent-color);
    }
    
    .citation-button.disabled {
      background-color: #f5f5f5;
      color: #999;
      cursor: not-allowed;
      opacity: 0.6;
    }
    
    .citation-button.disabled:hover {
      background-color: #f5f5f5;
      border-color: var(--border-color);
    }
    
    .tooltip {
      position: relative;
      display: inline-block;
    }
    
    .tooltip .tooltiptext {
      visibility: hidden;
      width: 200px;
      background-color: #333;
      color: #fff;
      text-align: center;
      border-radius: 4px;
      padding: 8px;
      position: absolute;
      z-index: 1;
      bottom: 125%;
      left: 50%;
      margin-left: -100px;
      opacity: 0;
      transition: opacity 0.3s;
      font-size: 12px;
    }
    
    .tooltip:hover .tooltiptext {
      visibility: visible;
      opacity: 1;
    }
    
    /* Interactive Architecture Styles */
    .interactive-architecture {
      position: relative;
      display: inline-block;
      border-radius: 8px;
      overflow: visible;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
    }
    
    .interactive-architecture img {
      width: 100%;
      border-radius: 8px;
      display: block;
    }
    
    .architecture-hotspot {
      position: absolute;
      background: #ffd700;
      border: 2px solid #000;
      border-radius: 50%;
      cursor: pointer;
      z-index: 10;
      width: 20px;
      height: 20px;
      margin-left: -10px;
      margin-top: -10px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
    }
    
    .architecture-hotspot:hover {
      background: #ffff00;
      border-color: #000;
      box-shadow: 0 2px 12px rgba(0, 0, 0, 0.4);
    }
    
    .architecture-hotspot.active {
      animation: color-glow-pulse 2s ease-in-out infinite alternate;
    }
    
    @keyframes color-glow-pulse {
      0% {
        background: #ffd700;
        border-color: #000;
        border-width: 2px;
        transform: scale(1);
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3);
      }
      100% {
        background: #ffa500;
        border-color: #ff0000;
        border-width: 3px;
        transform: scale(1.2);
        box-shadow: 
          0 0 15px rgba(255, 0, 0, 1),
          0 0 25px rgba(255, 0, 0, 0.8),
          0 0 35px rgba(255, 0, 0, 0.6),
          0 0 45px rgba(255, 0, 0, 0.4),
          0 0 55px rgba(255, 0, 0, 0.2);
      }
    }
    
    
    
    .architecture-hotspot::before {
      content: '';
      position: absolute;
      top: 50%;
      left: 50%;
      width: 8px;
      height: 8px;
      background: #8a2be2;
      border-radius: 50%;
      transform: translate(-50%, -50%);
    }
    
    .architecture-hotspot:hover::before {
      background: #9932cc;
    }
    
    .architecture-hotspot.active::before {
      background: #ff0000;
      box-shadow: 0 0 8px rgba(255, 0, 0, 0.8);
    }
    
    /* Dark blue buttons for specific components */
    .architecture-hotspot.dark-blue::before {
      background: #1e3a8a;
    }
    
    .architecture-hotspot.dark-blue:hover::before {
      background: #1e40af;
    }
    
    .architecture-hotspot.dark-blue.active::before {
      background: #ff0000;
      box-shadow: 0 0 8px rgba(255, 0, 0, 0.8);
    }
    
    /* Black buttons for specific components */
    .architecture-hotspot.black::before {
      background: #000000;
    }
    
    .architecture-hotspot.black:hover::before {
      background: #333333;
    }
    
    .architecture-hotspot.black.active::before {
      background: #ff0000;
      box-shadow: 0 0 8px rgba(255, 0, 0, 0.8);
    }
    
    /* Dark green buttons for specific components */
    .architecture-hotspot.dark-green::before {
      background: #166534;
    }
    
    .architecture-hotspot.dark-green:hover::before {
      background: #15803d;
    }
    
    .architecture-hotspot.dark-green.active::before {
      background: #ff0000;
      box-shadow: 0 0 8px rgba(255, 0, 0, 0.8);
    }
    
    /* Navy blue buttons for specific components */
    .architecture-hotspot.navy-blue::before {
      background: #1e40af;
    }
    
    .architecture-hotspot.navy-blue:hover::before {
      background: #2563eb;
    }
    
    .architecture-hotspot.navy-blue.active::before {
      background: #ff0000;
      box-shadow: 0 0 8px rgba(255, 0, 0, 0.8);
    }
    
    /* Deep green buttons for specific components */
    .architecture-hotspot.deep-green::before {
      background: #065f46;
    }
    
    .architecture-hotspot.deep-green:hover::before {
      background: #047857;
    }
    
    .architecture-hotspot.deep-green.active::before {
      background: #ff0000;
      box-shadow: 0 0 8px rgba(255, 0, 0, 0.8);
    }
    
    
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(20px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }
    
    
    
    .architecture-tooltip {
      position: absolute;
      background: linear-gradient(135deg, rgba(44, 62, 80, 0.95), rgba(52, 73, 94, 0.95));
      color: white;
      padding: 16px 20px;
      border-radius: 12px;
      font-size: 14px;
      line-height: 1.5;
      max-width: 280px;
      z-index: 1000;
      opacity: 0;
      visibility: hidden;
      transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
      pointer-events: none;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.1);
      transform: translateY(10px);
    }
    
    .architecture-tooltip.show {
      opacity: 1;
      visibility: visible;
      transform: translateY(0);
    }
    
    .architecture-tooltip strong {
      color: var(--accent-color);
      font-weight: 600;
      display: block;
      margin-bottom: 6px;
      font-size: 15px;
    }
    
    .architecture-tooltip::after {
      content: '';
      position: absolute;
      width: 0;
      height: 0;
      border: 8px solid transparent;
    }
    
    .architecture-tooltip.top::after {
      bottom: -16px;
      left: 50%;
      transform: translateX(-50%);
      border-top-color: rgba(44, 62, 80, 0.95);
    }
    
    .architecture-tooltip.bottom::after {
      top: -16px;
      left: 50%;
      transform: translateX(-50%);
      border-bottom-color: rgba(44, 62, 80, 0.95);
    }
    
    .architecture-tooltip.left::after {
      right: -16px;
      top: 50%;
      transform: translateY(-50%);
      border-left-color: rgba(44, 62, 80, 0.95);
    }
    
    .architecture-tooltip.right::after {
      left: -16px;
      top: 50%;
      transform: translateY(-50%);
      border-right-color: rgba(44, 62, 80, 0.95);
    }
    
    /* Information Panel Styles */
    .architecture-info-panel {
      flex: 1;
      min-width: 300px;
      max-width: 400px;
      background: linear-gradient(135deg, rgba(253, 253, 233, 0.95), rgba(253, 253, 233, 0.95));
      border: 1px solid var(--border-color);
      border-radius: 12px;
      padding: 20px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
      backdrop-filter: blur(10px);
      transition: all 0.3s ease;
      display: flex;
      flex-direction: column;
      justify-content: flex-start;
      align-self: stretch;
    }
    
    .architecture-info-panel h3 {
      color: var(--primary-color);
      font-size: 22px;
      font-weight: 600;
      margin: 0 0 12px 0;
      padding-bottom: 8px;
      border-bottom: 2px solid var(--text-color);
    }
    
    .architecture-info-panel p {
      color: var(--text-color);
      font-size: 16px;
      line-height: 1.6;
      margin: 0;
    }
    
    .architecture-info-panel .default-message {
      text-align: center;
      color: var(--secondary-color);
      font-style: italic;
      padding: 40px 20px;
    }
    
    .architecture-info-panel .default-message i {
      font-size: 24px;
      color: var(--accent-color);
      margin-bottom: 10px;
      display: block;
    }
    
    /* Video styling in info panel */
    .architecture-info-panel video {
      width: 100%;
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin: 10px 0;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    .architecture-info-panel .video-container {
      margin: 10px 0;
    }
    
    .architecture-info-panel .video-caption {
      font-size: 12px;
      color: var(--secondary-color);
      text-align: center;
      margin-top: 5px;
      font-style: italic;
    }
    
    .citation-button i {
      margin-right: 8px;
      font-size: 18px;
    }
    
    .citation-content {
      display: none;
      background-color: var(--light-gray);
      border: 1px solid var(--border-color);
      border-radius: 4px;
      padding: 10px;
      margin: 10px auto;
      max-width: 90%;
      position: relative;
      font-size: 14px;
      white-space: pre-wrap;
      word-break: break-word;
      text-align: left;
      font-family: Consolas, Monaco, 'Andale Mono', monospace;
    }
    
    .copy-button {
      margin-top: 10px;
      padding: 5px 10px;
      font-size: 12px;
      background-color: var(--accent-color);
      color: white;
      border: none;
      border-radius: 3px;
      cursor: pointer;
      transition: background-color 0.2s;
    }
    
    .copy-button:hover {
      background-color: var(--link-color);
    }
    
    /* Main content */
    .main-content {
      margin: 30px 0;
      padding: 0;
    }
    
    h2 {
      color: var(--primary-color);
      font-size: 24px;
            font-weight: 600;
      margin: 40px 0 20px;
      padding-bottom: 8px;
      border-bottom: 1px solid var(--border-color);
    }
    
    h3 {
      color: var(--primary-color);
      font-size: 20px;
      font-weight: 500;
      margin: 20px 0 15px;
    }
    
    p {
      margin-bottom: 20px;
      text-align: left;
      color: var(--text-color);
      line-height: 1.6;
    }
    
    /* Video container */
    .video-container {
      margin: 30px 0;
      text-align: center;
      width: 100%;
      border: none;
      background: transparent;
    }
    
    .video-container video {
      width: 100%;
      max-width: 100%;
      border: none;
      border-radius: 0;
      box-shadow: none;
      background: transparent;
    }
    
    .figure-caption {
      margin-top: 10px;
      font-size: 14px;
      color: var(--secondary-color);
      text-align: center;
    }
    
    /* Grid layout */
    .grid-container {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 25px 0;
    }
    
    .grid-item {
      background-color: white;
      border: 1px solid var(--border-color);
      border-radius: 4px;
      padding: 20px;
      transition: transform 0.2s, box-shadow 0.2s;
    }
    
    .grid-item:hover {
      transform: translateY(-3px);
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    }
    
    .grid-item h3 {
      color: var(--primary-color);
      margin-top: 0;
      text-align: left;
      border-bottom: 1px solid var(--border-color);
      padding-bottom: 10px;
      margin-bottom: 15px;
    }
    
    /* Capability sections */
    .capability-container {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 25px 0;
    }
    
    .capability-section {
      background-color: white;
      border: 1px solid var(--border-color);
      border-radius: 4px;
    }
    
    .capability-title {
      color: var(--primary-color);
      background-color: var(--light-gray);
      margin: 0;
      text-align: center;
      padding: 12px 0;
      font-size: 18px;
      font-weight: 500;
    }
    
    .capability-list {
      padding: 15px 15px 15px 35px;
      margin: 0;
      list-style-type: disc;
    }
    
    .capability-list li {
      padding: 4px 0;
      color: var(--text-color);
      font-size: 15px;
    }
    
    /* Book showcase */
    .book-showcase {
      display: flex;
      justify-content: center;
      gap: 30px;
      margin: 25px 0;
      flex-wrap: wrap;
    }
    
    .book-item {
      text-align: center;
      max-width: 250px;
    }
    
    .book-item img {
      width: 100%;
      border: none;
      border-radius: 0;
      transition: transform 0.2s;
      background: white;
    }
    
    .book-item img:hover {
      transform: scale(1.03);
    }
    
    .book-item p {
      margin-top: 10px;
      font-weight: 500;
      text-align: center;
      color: var(--secondary-color);
    }
    
    /* Figures */
    .figure {
      margin: 30px 0;
      text-align: center;
      width: 100%;
      background: white;
      border: none;
      padding: 0;
    }
    
    .figure img {
      width: 100%;
      max-width: 100%;
      border: none;
      border-radius: 0;
      box-shadow: none;
      background: white;
    }
    
    /* Section divider */
    .section-divider {
      height: 1px;
      background: var(--border-color);
      margin: 40px 0;
      border: none;
    }
    
    /* Footer */
        .footer {
      text-align: center;
      padding: 20px 0;
      margin-top: 40px;
      color: var(--secondary-color);
      font-size: 14px;
      border-top: 1px solid var(--border-color);
    }
    
    /* Placeholder styles */
    .placeholder {
      background-color: white;
      border: none;
      border-radius: 0;
      padding: 20px;
      text-align: left;
      color: #000000;
      font-style: justify;
      margin: 20px 0;
    }
    
    /* Interactive Demo Styles */
    .interactive-demo {
      margin: 40px 0;
      background: white;
      border: 1px solid var(--border-color);
      border-radius: 8px;
      overflow: hidden;
      max-width: none;
      width: 100%;
    }
    
    .demo-header {
      background: var(--light-gray);
      padding: 20px 60px;
      border-bottom: 1px solid var(--border-color);
    }
    
    .demo-title {
      color: var(--primary-color);
      font-size: 1.3rem;
      font-weight: 600;
      margin-bottom: 10px;
    }
    
    .demo-description {
      color: var(--secondary-color);
      font-size: 0.9rem;
      line-height: 1.5;
    }
    
    .demo-controls {
      padding: 20px 60px;
      background: #f8f9fa;
      border-bottom: 1px solid var(--border-color);
      display: flex;
      gap: 15px;
      align-items: center;
      flex-wrap: wrap;
    }
    
    .demo-button {
      padding: 8px 16px;
      background: var(--accent-color);
      color: white;
      border: none;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.9rem;
      transition: background-color 0.2s;
    }
    
    .demo-button:hover {
      background: var(--link-color);
    }
    
    .demo-button:disabled {
      background: #ccc;
      cursor: not-allowed;
    }
    
    .demo-button.simulation-button:hover {
      background: linear-gradient(135deg, #5a6fd8 0%, #6a4190 100%);
    }
    
    .demo-button.robot-button:hover {
      background: linear-gradient(135deg, #ee82f0 0%, #f3455a 100%);
    }
    
    .step-counter {
      color: var(--secondary-color);
            font-size: 0.9rem;
      margin-left: auto;
    }
    
    .demo-content {
      padding: 0;
      height: 70vh;
      overflow-y: auto;
      border: 1px solid var(--border-color);
      background: white;
      max-width: none;
      width: 100%;
    }
    
    .demo-step {
      padding: 30px 60px;
      border-bottom: 1px solid #f0f0f0;
      display: none;
      height: 100%;
      overflow-y: auto;
    }
    
    .demo-step.active {
      display: block !important;
      background: #fff9e6;
    }
    
    .demo-step.completed {
      display: none !important;
    }
    
    /* Consistent font styling for all demo steps */
    .demo-step .step-content {
      font-size: 14px;
      line-height: 1.6;
    }
    
    .demo-step .step-content h4 {
      color: var(--accent-color);
      font-size: 18px;
      margin-bottom: 15px;
    }
    
    .demo-step .step-content ul {
      font-size: 14px;
      line-height: 1.6;
      margin-bottom: 25px;
    }
    
    .demo-step .step-content li {
      margin-bottom: 6px;
      padding-left: 8px;
    }
    
    .demo-step .step-content strong {
      color: var(--primary-color);
    }
    
    /* Glossary specific styling */
    .glossary-term {
      background: white;
      border-left: 3px solid var(--accent-color);
      border-radius: 6px;
      padding: 12px 15px;
      margin-bottom: 8px;
      box-shadow: 0 1px 4px rgba(0,0,0,0.08);
      transition: all 0.2s ease;
      border: 1px solid rgba(0,0,0,0.05);
    }
    
    
    .glossary-term .glossary-badge {
      background: var(--accent-color);
      color: white;
      border-radius: 3px;
      width: 24px;
      height: 24px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: bold;
      font-size: 11px;
      flex-shrink: 0;
      margin-top: 1px;
    }
    
    .glossary-term .glossary-content {
      flex: 1;
    }
    
    .glossary-term .glossary-title {
      color: var(--primary-color);
      font-size: 16px;
      font-weight: 600;
      margin: 0 0 4px 0;
      line-height: 1.2;
    }
    
    .glossary-term .glossary-definition {
      color: var(--text-color);
      margin: 0;
      font-size: 13px;
      line-height: 1.4;
    }
    
    /* Alphabet directory styling */
    .alphabet-btn {
      background: white;
      color: var(--text-color);
      border: 2px solid var(--border-color);
      border-radius: 6px;
      padding: 8px 12px;
      font-size: 14px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.2s ease;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    .alphabet-btn:hover {
      background: var(--accent-color);
      color: white;
      border-color: var(--accent-color);
      transform: translateY(-1px);
      box-shadow: 0 4px 8px rgba(0,0,0,0.15);
    }
    
    .alphabet-btn.active {
      background: var(--accent-color);
      color: white;
      border-color: var(--accent-color);
      box-shadow: 0 4px 8px rgba(0,0,0,0.15);
    }
    
    /* Alphabet section headers */
    .alphabet-section {
      margin-bottom: 30px;
    }
    
    .alphabet-header {
      background: linear-gradient(135deg, var(--accent-color), var(--primary-color));
      color: white;
      padding: 12px 20px;
      border-radius: 8px;
      font-size: 18px;
      font-weight: 700;
      margin-bottom: 15px;
      text-align: center;
      box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }
    
    /* References formatting */
    #references-container ol li {
      margin-bottom: 8px;
      padding-bottom: 4px;
    }
    
    #references-container ol li:last-child {
      margin-bottom: 0;
    }
    
    /* Custom scrollbar styling */
    #references-container::-webkit-scrollbar,
    #glossary-container::-webkit-scrollbar {
      width: 8px;
    }
    
    #references-container::-webkit-scrollbar-track,
    #glossary-container::-webkit-scrollbar-track {
      background: #f1f1f1;
      border-radius: 4px;
    }
    
    #references-container::-webkit-scrollbar-thumb,
    #glossary-container::-webkit-scrollbar-thumb {
      background: var(--accent-color);
      border-radius: 4px;
    }
    
    #references-container::-webkit-scrollbar-thumb:hover,
    #glossary-container::-webkit-scrollbar-thumb:hover {
      background: var(--primary-color);
    }
    
    /* Firefox scrollbar styling */
    #references-container,
    #glossary-container {
      scrollbar-width: thin;
      scrollbar-color: var(--accent-color) #f1f1f1;
    }
    
    .step-number {
      display: inline-block;
      background: var(--accent-color);
      color: white;
      width: 24px;
      height: 24px;
      border-radius: 50%;
      text-align: center;
      line-height: 24px;
      font-size: 0.8rem;
      font-weight: 600;
      margin-right: 10px;
    }
    
    .step-title {
      color: var(--primary-color);
      font-weight: 600;
      margin-bottom: 10px;
      display: flex;
      align-items: center;
    }
    
    .step-content {
      margin-left: 20px;
    }
    
    .human-input {
      background: #e3f2fd;
      border-left: 4px solid #2196f3;
      padding: 10px 15px;
      margin: 10px 0;
      border-radius: 0 4px 4px 0;
      font-style: italic;
    }
    
    .robot-response {
      background: #f3e5f5;
      border-left: 4px solid #9c27b0;
      padding: 10px 15px;
      margin: 10px 0;
      border-radius: 0 4px 4px 0;
    }
    
    .code-block {
      background: #f5f5f5;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 15px;
      margin: 10px 0;
      font-family: 'Courier New', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
      overflow-x: auto;
      width: calc(100% - 1px);
      max-width: none;
    }
    
    pre.code-block {
      background: #f5f5f5;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 15px;
      margin: 10px 0;
      font-family: 'Courier New', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
      overflow-x: auto;
      white-space: pre-wrap;
      word-wrap: break-word;
      width: calc(100% - 10px);
      max-width: none;
    }
    
    pre.code-block code {
      background: transparent;
      border: none;
      padding: 0;
      font-family: inherit;
      font-size: inherit;
      color: inherit;
    }
    
    .ontology-info {
      background: #fff3e0;
      border-left: 4px solid #ff9800;
      padding: 10px 15px;
      margin: 10px 0;
      border-radius: 0 4px 4px 0;
    }
    
    .plan-status {
      display: inline-block;
      padding: 2px 8px;
      border-radius: 12px;
      font-size: 0.8rem;
      font-weight: 500;
      margin-left: 10px;
    }
    
    .status-running { background: #e3f2fd; color: #1976d2; }
    .status-pending { background: #fff3e0; color: #f57c00; }
    .status-finished { background: #e8f5e8; color: #388e3c; }
    .status-impassed { background: #ffebee; color: #d32f2f; }

    /* Step video styling */
    .step-video {
      border: 1px solid var(--border-color);
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    
    /* Video grid container for step videos */
    .video-grid-container {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      align-items: start;
      margin: 20px 0;
    }
    
    .video-stack-container {
      display: flex;
      flex-direction: column;
      gap: 15px;
    }
    
    /* Image grid container for step images */
    .image-grid-container {
      display: flex;
      flex-direction: column;
      gap: 20px;
      align-items: center;
      margin: 20px 0;
    }
    
    .robot-images-container {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      width: 100%;
      max-width: 800px;
    }
    
    .image-container img {
      transition: transform 0.2s;
    }
    
    .image-container img:hover {
      transform: scale(1.02);
    }
    
    /* Demo full buttons styling */
    .demo-full-button {
      padding: 15px 30px;
      border: none;
      border-radius: 8px;
      font-size: 1.1rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      text-transform: none;
      min-width: 200px;
    }
    
    .simulation-button {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    
    .simulation-button:hover {
      box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }
    
    .robot-button {
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
      color: white;
      box-shadow: 0 4px 15px rgba(240, 147, 251, 0.3);
    }
    
    .robot-button:hover {
      box-shadow: 0 6px 20px rgba(240, 147, 251, 0.4);
    }
    
    .demo-buttons-container {
      margin: 30px 0;
      text-align: center;
      display: flex;
      gap: 20px;
      justify-content: center;
      flex-wrap: wrap;
    }
    
    /* Full-width demo video styling */
    .full-width-video-container {
      margin: 30px 0;
      width: 100%;
    }
    
    .full-demo-video {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
    }
    

    /* Step Navigation Styles */
    .step-navigation {
      background: #f8f9fa;
      border-bottom: 1px solid var(--border-color);
      padding: 15px 60px;
    }
    
    .step-nav-title {
      font-weight: 600;
      color: var(--primary-color);
      margin-bottom: 10px;
      font-size: 0.9rem;
    }
    
    .step-nav-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(40px, 1fr));
      gap: 12px;
      max-width: 100%;
    }
    
    .step-nav-button {
      background: white;
      border: 1px solid var(--border-color);
      border-radius: 4px;
      padding: 8px 4px;
      font-size: 0.8rem;
      font-weight: 500;
      color: var(--text-color);
      cursor: pointer;
      transition: all 0.2s;
      text-align: center;
      min-width: 40px;
    }
    
    .step-nav-button:hover {
      background: var(--light-gray);
      border-color: var(--accent-color);
    }
    
    .step-nav-button.active {
      background: var(--accent-color);
      color: white;
      border-color: var(--accent-color);
    }
    
    .step-nav-button.completed {
      background: #e8f5e8;
      color: #388e3c;
      border-color: #4caf50;
    }
    
    /* Special demo navigation buttons */
    .simulation-nav-button {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      border-color: #667eea;
      font-size: 0.8rem;
      font-weight: 600;
      padding: 8px 4px;
      min-width: 40px;
    }
    
    .simulation-nav-button:hover {
      background: linear-gradient(135deg, #5a6fd8 0%, #6a4190 100%);
      box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
    }
    
    .simulation-nav-button.active {
      background: linear-gradient(135deg, #5a6fd8 0%, #6a4190 100%);
      box-shadow: 0 4px 12px rgba(102, 126, 234, 0.6);
    }
    
    .robot-nav-button {
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
      color: white;
      border-color: #f093fb;
      font-size: 0.8rem;
      font-weight: 600;
      padding: 8px 4px;
      min-width: 40px;
    }
    
    .robot-nav-button:hover {
      background: linear-gradient(135deg, #ee82f0 0%, #f3455a 100%);
      box-shadow: 0 4px 12px rgba(240, 147, 251, 0.4);
    }
    
    .robot-nav-button.active {
      background: linear-gradient(135deg, #ee82f0 0%, #f3455a 100%);
      box-shadow: 0 4px 12px rgba(240, 147, 251, 0.6);
    }

    /* Fixed Top Navigation Bar */
    .top-nav {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: rgba(255, 255, 255, 0.95);
      backdrop-filter: blur(10px);
      border-bottom: 1px solid var(--border-color);
      z-index: 1000;
      transition: all 0.3s ease;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }
    
    .nav-container {
      max-width: 1400px;
      margin: 0 auto;
      padding: 0 20px;
      display: flex;
      justify-content: space-between;
      align-items: center;
      height: 60px;
    }
    
    .nav-logo {
      font-size: 20px;
      font-weight: 700;
      color: var(--primary-color);
      text-decoration: none;
    }
    
    .nav-menu {
      display: flex;
      list-style: none;
      margin: 0;
      padding: 0;
      gap: 30px;
    }
    
    .nav-link {
      color: var(--text-color);
      text-decoration: none;
      font-weight: 500;
      font-size: 16px;
      padding: 8px 12px;
      border-radius: 4px;
      transition: all 0.3s ease;
      position: relative;
    }
    
    .nav-link:hover {
      color: var(--accent-color);
      background-color: var(--light-gray);
    }
    
    .nav-link.active {
      color: var(--accent-color);
      background-color: rgba(52, 152, 219, 0.1);
    }
    
    .nav-toggle {
      display: none;
      flex-direction: column;
      cursor: pointer;
      gap: 4px;
    }
    
    .nav-toggle .bar {
      width: 25px;
      height: 3px;
      background-color: var(--primary-color);
      transition: all 0.3s ease;
      border-radius: 2px;
    }
    
    /* Mobile Navigation */
    @media (max-width: 768px) {
      .nav-menu {
        position: fixed;
        top: 60px;
        left: -100%;
        width: 100%;
        height: calc(100vh - 60px);
        background-color: rgba(255, 255, 255, 0.98);
        backdrop-filter: blur(10px);
        flex-direction: column;
        justify-content: flex-start;
        align-items: center;
        padding-top: 50px;
        gap: 20px;
        transition: left 0.3s ease;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      }
      
      .nav-menu.active {
        left: 0;
      }
      
      .nav-link {
        font-size: 16px;
        padding: 15px 20px;
        width: 80%;
        text-align: center;
        border-radius: 8px;
      }
      
      .nav-toggle {
        display: flex;
      }
      
      .nav-toggle.active .bar:nth-child(1) {
        transform: rotate(45deg) translate(5px, 5px);
      }
      
      .nav-toggle.active .bar:nth-child(2) {
        opacity: 0;
      }
      
      .nav-toggle.active .bar:nth-child(3) {
        transform: rotate(-45deg) translate(7px, -6px);
      }
    }
    
    /* Add top padding to body to account for fixed nav */
    body {
      padding-top: 60px;
    }

    /* Responsive adjustments */
        @media (max-width: 768px) {
      .paper-title {
        font-size: 26px;
      }
      
      .authors {
        font-size: 16px;
      }
      
      .grid-container, 
      .capability-container {
        grid-template-columns: 1fr;
      }
      
      .citation-content {
        max-width: 100%;
        font-size: 12px;
      }
      
      .demo-controls {
        flex-direction: column;
        align-items: stretch;
      }
      
      .step-counter {
        margin-left: 0;
        margin-top: 10px;
            }
      
      .step-nav-grid {
        grid-template-columns: repeat(auto-fit, minmax(35px, 1fr));
        gap: 8px;
      }
      
      .step-nav-button {
        padding: 6px 2px;
        font-size: 0.75rem;
        min-width: 35px;
      }
      
      .simulation-nav-button,
      .robot-nav-button {
        min-width: 35px;
        font-size: 0.75rem;
        padding: 6px 2px;
      }
      
      .demo-content {
        height: 60vh;
      }
      
      .video-grid-container {
        grid-template-columns: 1fr;
        gap: 15px;
      }
      
      .video-stack-container {
        gap: 10px;
      }
      
      .robot-images-container {
        grid-template-columns: 1fr;
        gap: 15px;
      }
      
      .image-grid-container {
        gap: 15px;
      }
      
      .demo-buttons-container {
        flex-direction: column;
        gap: 15px;
      }
      
      .demo-full-button {
        min-width: 180px;
        padding: 12px 25px;
        font-size: 1rem;
      }
        }
        
        /* Image Modal Styles */
        .image-modal {
          display: none;
          position: fixed;
          z-index: 1000;
          left: 0;
          top: 0;
          width: 100%;
          height: 100%;
          background-color: rgba(0, 0, 0, 0.8);
          backdrop-filter: blur(5px);
        }
        
        .image-modal-content {
          position: absolute;
          top: 50%;
          left: 50%;
          transform: translate(-50%, -50%);
          max-width: 75%;
          max-height: 75%;
          background: white;
          border-radius: 12px;
          box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
          overflow: hidden;
          display: flex;
          flex-direction: column;
        }
        
        .image-modal img {
          width: 100%;
          height: auto;
          display: block;
          max-height: 60vh;
          object-fit: contain;
        }
        
        .image-modal-caption {
          padding: 15px 20px;
          background: #f8f9fa;
          border-top: 1px solid #e9ecef;
          font-size: 0.9em;
          color: #666;
          font-style: italic;
        }
        
        .image-modal-close {
          position: absolute;
          top: 10px;
          right: 15px;
          color: white;
          font-size: 28px;
          font-weight: bold;
          cursor: pointer;
          background: rgba(0, 0, 0, 0.5);
          border-radius: 50%;
          width: 40px;
          height: 40px;
          display: flex;
          align-items: center;
          justify-content: center;
          transition: background-color 0.3s;
        }
        
        .image-modal-close:hover {
          background: rgba(0, 0, 0, 0.7);
        }
        
        .clickable-image {
          cursor: pointer;
          transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .clickable-image:hover {
          transform: scale(1.02);
          box-shadow: 0 4px 12px rgba(0,0,0,0.15) !important;
        }
        
        /* Video Modal Styles */
        .video-modal {
          display: none;
          position: fixed;
          z-index: 1000;
          left: 0;
          top: 0;
          width: 100%;
          height: 100%;
          background-color: rgba(0, 0, 0, 0.9);
          backdrop-filter: blur(5px);
        }
        
        .video-modal-content {
          position: absolute;
          top: 50%;
          left: 50%;
          transform: translate(-50%, -50%);
          max-width: 90%;
          max-height: 90%;
          background: black;
          border-radius: 12px;
          box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
          overflow: hidden;
        }
        
        .video-modal video {
          width: 100%;
          height: auto;
          display: block;
        }
        
        .video-modal-caption {
          padding: 15px 20px;
          background: #1a1a1a;
          border-top: 1px solid #333;
          font-size: 0.9em;
          color: #ccc;
          font-style: italic;
        }
        
        .video-modal-close {
          position: absolute;
          top: 10px;
          right: 15px;
          color: white;
          font-size: 28px;
          font-weight: bold;
          cursor: pointer;
          background: rgba(0, 0, 0, 0.7);
          border-radius: 50%;
          width: 40px;
          height: 40px;
          display: flex;
          align-items: center;
          justify-content: center;
          transition: background-color 0.3s;
          z-index: 1001;
        }
        
        .video-modal-close:hover {
          background: rgba(0, 0, 0, 0.9);
        }
        
        .clickable-video {
          cursor: pointer;
        }
        
    </style>
</head>
<body>
    <!-- Fixed Top Navigation Bar -->
    <nav class="top-nav">
        <div class="nav-container">
            <div class="nav-logo">
                <span>HARMONIC</span>
            </div>
            <ul class="nav-menu">
                <li><a href="#overview" class="nav-link">Overview</a></li>
                <li><a href="#architecture" class="nav-link">Architecture</a></li>
                <li><a href="#evaluation" class="nav-link">Evaluation-1</a></li>
                <li><a href="#evaluation2" class="nav-link">Evaluation-2</a></li>
                <li><a href="#presentation" class="nav-link">Presentation</a></li>
                <li><a href="#references" class="nav-link">References</a></li>
                <li><a href="#glossary" class="nav-link">Glossary</a></li>
            </ul>
            <div class="nav-toggle">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <div class="container">
    <div class="header">
      <div class="paper-title">
        HARMONIC: A Content-Centric Cognitive Robotic Architecture
      </div>
      
      <!-- Authors Section - REMOVED FOR BLINDED SUBMISSION -->
      <div class="authors">
        <div class="placeholder" style="background-color: #fff3cd; border-color: #ffeaa7; color: #856404;">
          <strong>Authors and affiliations removed for double-blind submission</strong>
        </div>
      </div>
      
      <!-- Citation Section -->
      <div class="citation-container">
        <div class="tooltip">
          <a href="#" class="citation-button disabled" onclick="return false;">
            <i class="ai ai-arxiv"></i> Pre-print
          </a>
          <span class="tooltiptext">Available after the completion of review</span>
        </div>
        <div class="tooltip">
          <button class="citation-button disabled" onclick="return false;">
            <i class="fas fa-quote-right"></i> BibTeX
          </button>
          <span class="tooltiptext">Available after the completion of review</span>
        </div>
        <div class="tooltip">
          <button class="citation-button disabled" onclick="return false;">
            <i class="fas fa-book"></i> Citation
          </button>
          <span class="tooltiptext">Available after the completion of review</span>
        </div>
      </div>
      
      <!-- BibTeX Citation - BLINDED FOR SUBMISSION -->
      <div id="bibtex-citation" class="citation-content">
        @misc{harmonic2026contentcentriccognitive,
          title={HARMONIC: A Content-Centric Cognitive Robotic Architecture}, 
          author={[Authors removed for blinded submission]},
          year={2026},
          eprint={[arXiv ID will be added after acceptance]},
          archivePrefix={arXiv},
          primaryClass={cs.RO},
          url={[URL will be added after acceptance]}, 
        }
        <button class="copy-button" style="margin-left: 700px;" onclick="copyText('bibtex-citation')">Copy</button>
      </div>
      
      <!-- Citation Formats -->
      <div id="citation-formats" class="citation-content" style="display: none; background-color: #f5f5f5; border-radius: 4px;">
        <div style="margin-bottom: -65px;">
          <div style="font-weight: bold; margin-bottom: -50px;margin-top: -90px;">APA:</div>
          <div id="apa-text" style="margin-bottom: -50px;"></div><br>
          <button class="copy-button" style="margin-left: 700px;" onclick="copyCitationFormat('apa-text')">Copy</button>
        </div>
        <hr style="margin: 5px 0; border: none; border-top: 1px solid #ccc;">
        <div style="margin-bottom: -65px;">
          <div style="font-weight: bold; margin-bottom: 0px; margin-bottom: -50px;margin-top: -90px;">MLA:</div>
          <div id="mla-text" style="margin-bottom: -50px;"></div><br>
          <button class="copy-button"style="margin-left: 700px;" onclick="copyCitationFormat('mla-text')">Copy</button>
        </div>
        <hr style="margin: 5px 0; border: none; border-top: 1px solid #ccc;">
        <div style="margin-bottom: -65px;">
          <div style="font-weight: bold; margin-bottom: -50px;margin-top: -90px;">Chicago:</div>
          <div id="chicago-text" style="margin-bottom: -50px;"></div><br>
          <button class="copy-button" style="margin-left: 700px;" onclick="copyCitationFormat('chicago-text')">Copy</button>
        </div>
      </div>
    </div>
    
    <div class="main-content">
      <!-- Overview Section - TO BE CUSTOMIZED -->
      <section id="overview">
      <h2>Overview</h2>
        <div class="placeholder" style="text-align: justify;">
          This paper presents HARMONIC (Human-AI Robotic Team Member Operating with Natural Intelligence and Communication), a novel cognitive-robotic architecture that fundamentally reimagines how robots can collaborate with humans in complex, real-world scenarios. Unlike current foundation-model approaches that suffer from hallucinations, lack of transparency, and data scarcity issues, HARMONIC integrates a mature cognitive architecture (OntoAgent) with robotic control systems to create a dual-layer framework. The strategic layer performs high-level reasoning using explicit, inspectable knowledge representations, while the tactical layer handles real-time execution through behavior trees and specialized robotic controllers. This design enables robots to engage in transparent, explainable decision-making, natural language communication, and safe physical execution across diverse environments. We demonstrate HARMONIC's capabilities through two comprehensive proof-of-concept systems: a shipboard maintenance assistant that collaborates with human mechanics to diagnose problems and retrieve replacement parts, and a multi-robot search team that coordinates heterogeneous robots (UGV and drone) to locate lost objects in apartment environments. Both systems operate seamlessly across high-fidelity simulation and physical robotic platforms, showcasing HARMONIC's ability to bridge the gap between symbolic reasoning and embodied action while maintaining the transparency and trust essential for real-world human-robot collaboration.
        </div>
      </section>
      
      <!-- Architecture Section -->
      <section id="architecture">
      <h2>Architecture</h2>
      <div style="text-align: right; margin-bottom: 20px; color: var(--accent-color); font-weight: 600;">
        <span style="color: var(--accent-color);">💡 Interactive:</span> 
        <span style="color: var(--text-color);">Click on the hotspot areas to explore each component in detail.</span>
      </div>
      <div class="figure" style="display: flex; align-items: stretch; gap: 20px; flex-wrap: wrap;">
        <div class="interactive-architecture" style="flex: 0 0 auto; max-width: 70%;">
          <img src="assets/images/architecture/OverviewPicture1.jpg" alt="HARMONIC System Architecture">
          
          <!-- External Top Blocks -->
          <div class="architecture-hotspot dark-blue" style="top: 3%; left: 16.5%;" data-component="long-horizon-planning"></div>
          
          <div class="architecture-hotspot dark-blue" style="top: 3%; left: 53.25%;" data-component="metacognition"></div>
          
          <div class="architecture-hotspot dark-blue" style="top: 3%; left: 76%;" data-component="explainability"></div>
          
          <!-- System 2: Strategic Layer -->
          <div class="architecture-hotspot black" style="top: 26.25%; left: 28.5%;" data-component="system2-layer"></div>
          <div class="architecture-hotspot navy-blue" style="top: 33.5%; left: 26.5%;" data-component="ontoagent-framework"></div>
          
          <!-- OntoAgent Components -->
          <div class="architecture-hotspot navy-blue" style="top: 41.5%; left: 16%;" data-component="perception-interpretation"></div>
          <div class="architecture-hotspot navy-blue" style="top: 41.5%; left: 35%;" data-component="attention-service-s2"></div>
          <div class="architecture-hotspot navy-blue" style="top: 41.5%; left: 51%;" data-component="strategic-reasoning"></div>
          <div class="architecture-hotspot navy-blue" style="top: 41.5%; left: 73.5%;" data-component="rendering-services"></div>
          
          <!-- Communication APIs -->
          <div class="architecture-hotspot black" style="top: 55%; left: 55%;" data-component="perception-apis"></div>
          <div class="architecture-hotspot black" style="top: 55%; left: 74%;" data-component="action-apis"></div>
          
          <!-- System 1: Tactical Layer -->
          <div class="architecture-hotspot black" style="top: 84%; left: 30%;" data-component="system1-layer"></div>
          
          <!-- System 1 Components -->
          <div class="architecture-hotspot deep-green" style="top: 70%; left: 16%;" data-component="perception-services"></div>
          <div class="architecture-hotspot deep-green" style="top: 70%; left: 35%;" data-component="attention-service-s1"></div>
          <div class="architecture-hotspot deep-green" style="top: 70%; left: 52%;" data-component="tactical-reasoning"></div>
          <!-- Behavior Trees (activated) -->
          <div class="architecture-hotspot deep-green" style="top: 78.5%; left: 54%;" data-component="behavior-trees"></div>
          <div class="architecture-hotspot deep-green" style="top: 70%; left: 74%;" data-component="effector-services"></div>
          
          <!-- Perception Inputs (Left Side) -->
          <div class="architecture-hotspot black" style="top: 45%; left: 2.5%;" data-component="perception-inputs"></div>
          <!-- <div class="architecture-hotspot" style="top: 45%; left: 5%;" data-component="environment-perception"></div>
          <div class="architecture-hotspot" style="top: 50%; left: 5%;" data-component="speech-input"></div>
          <div class="architecture-hotspot" style="top: 55%; left: 5%;" data-component="health-monitoring"></div> -->
          
          <!-- Action Outputs (Right Side) -->
          <div class="architecture-hotspot black" style="top: 43%; left: 90%;" data-component="action-outputs"></div>
          <!-- <div class="architecture-hotspot" style="top: 45%; left: 88%;" data-component="motor-control"></div>
          <div class="architecture-hotspot" style="top: 50%; left: 88%;" data-component="actuator-control"></div>
          <div class="architecture-hotspot" style="top: 55%; left: 88%;" data-component="audio-output"></div> -->
          
          <!-- Bottom External Modules -->
          <div class="architecture-hotspot dark-green" style="top: 95.5%; left: 10%;" data-component="perception-mapping"></div>
          <div class="architecture-hotspot dark-green" style="top: 95.5%; left: 28.9%;" data-component="reactive-planning"></div>
          <div class="architecture-hotspot dark-green" style="top: 95.5%; left: 55%;" data-component="short-horizon-plans"></div>
          <div class="architecture-hotspot dark-green" style="top: 95.5%; left: 73%;" data-component="actuation-policies"></div>
        </div>
        
        <!-- Information Panel -->
        <div class="architecture-info-panel">
          <div id="info-panel-content">
            <div class="default-message">
              <i class="fas fa-hand-pointer"></i>
              <strong>Interactive Architecture</strong><br>
              Click on the hotspot areas to learn more about each component of the HARMONIC system.
            </div>
          </div>
        </div>
        
        <div class="figure-caption">
          <p>An overview of the HARMONIC framework, showing the Strategic and Tactical components representing the high-level planning (System 2) and low-level execution (System 1), respectively.</p>
        </div>
        
      </div>
      </section>
      
      
      <!-- Evaluation Results Section - TO BE CUSTOMIZED -->
      <section id="evaluation">
      <h2>Evaluation - 1 (Shipboard maintenance)</h2>
      <div class="interactive-demo">
        <div class="demo-header">
          <div class="demo-title">Step-by-Step HARMONIC Reasoning Process</div>
          <div class="demo-description">
            This interactive demonstration shows how HARMONIC processes natural language input, 
            reasons about the situation, and executes multi-step plans. <strong>Click any step number above to jump directly to that part of the process</strong>, or use the Previous/Next buttons to go through sequentially. Each step shows the detailed cognitive reasoning behind HARMONIC's decision-making.
          </div>
          <div class="scenario-description" style="margin-top: 25px; padding: 20px; background: linear-gradient(135deg, rgba(52, 152, 219, 0.08) 0%, rgba(155, 89, 182, 0.08) 100%); border-radius: 12px; border: 1px solid rgba(52, 152, 219, 0.2); box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05); position: relative; overflow: hidden;">
            <div style="position: absolute; top: 0; left: 0; width: 4px; height: 100%; background: linear-gradient(180deg, var(--accent-color) 0%, #9b59b6 100%);"></div>
            <div style="position: relative; z-index: 1;">
              <div style="display: flex; align-items: center; margin-bottom: 12px;">
                <div style="width: 12px; height: 12px; background: linear-gradient(135deg, var(--accent-color) 0%, #9b59b6 100%); border-radius: 3px; margin-right: 12px; transform: rotate(45deg); box-shadow: 0 2px 6px rgba(52, 152, 219, 0.3);"></div>
                <h4 style="margin: 0; color: var(--accent-color); font-size: 16px; font-weight: 600; letter-spacing: 0.5px;">SCENARIO 1</h4>
              </div>
              <p style="margin: 0; font-style: italic; color: var(--text-color); line-height: 1.7; font-size: 15px; text-align: justify;">
                In a shipboard maintenance team, the robot assistant, <strong style="color: var(--accent-color); font-style: normal;">LEIA</strong>, interacts conversationally with the maintenance mechanic, <strong style="color: var(--accent-color); font-style: normal;">Daniel</strong>, assists in diagnosing engine issues, and supports maintenance tasks by locating and retrieving necessary replacement parts.
              </p>
            </div>
          </div>
        </div>
        
        <div class="demo-controls">
          <button class="demo-button" id="prevStep" onclick="previousStep()">← Previous</button>
          <button class="demo-button" id="nextStep" onclick="nextStep()">Next →</button>
          <button class="demo-button" id="autoPlayBtn" onclick="toggleAutoPlay()">Auto Play</button>
          <button class="demo-button simulation-button" onclick="goToStep(29)" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none;">
            Watch Simulation Demo
          </button>
          <button class="demo-button robot-button" onclick="goToStep(30)" style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; border: none;">
            Watch Real-Robot Demo
          </button>
          <div class="step-counter">
            Step <span id="currentStep">0</span> of <span id="totalSteps">30</span>
          </div>
        </div>
        
        <!-- Step Navigation Menu -->
        <div class="step-navigation">
          <div class="step-nav-title">Jump to Step:</div>
          <div class="step-nav-grid" id="stepNavGrid">
            <!-- Step navigation buttons will be generated by JavaScript -->
          </div>
        </div>
        
        <div class="demo-content" id="demoContent">
          <!-- Introduction -->
          <div class="demo-step active" id="step0" style="display: none;">
            <div class="step-title">
              <span class="step-number">0</span>
              Understanding HARMONIC's Reasoning Process
            </div>
            <div class="step-content">
              <p><strong>Welcome to the HARMONIC Interactive Demo!</strong></p>
              <p>This demonstration shows how HARMONIC processes a real-world scenario: a shipboard maintenance situation where a human reports an engine overheating problem and requests help.</p>
              
              <h4 style="color: var(--accent-color); font-size: 18px; margin-bottom: 15px;">What You'll See:</h4>
              <ul style="font-size: 14px; line-height: 1.6; margin-bottom: 25px;">
                <li style="margin-bottom: 6px; padding-left: 8px;"><strong style="color: var(--primary-color);">Natural Language Understanding:</strong> <span style="color: var(--text-color);">The system interprets human utterances through semantic parsing, converting unstructured speech into formal meaning representations (TMRs) grounded in the system's ontological framework</span></li>
                <li style="margin-bottom: 6px; padding-left: 8px;"><strong style="color: var(--primary-color);">Dialog Management:</strong> <span style="color: var(--text-color);">The system employs discourse protocols and adjacency pair patterns to maintain conversational coherence, selecting contextually appropriate communicative acts based on dialog state and team dynamics</span></li>
                <li style="margin-bottom: 6px; padding-left: 8px;"><strong style="color: var(--primary-color);">Knowledge-Based Reasoning:</strong> <span style="color: var(--text-color);">The system performs ontological inference operations, traversing hierarchical knowledge structures to retrieve relevant facts, scripts, and procedural knowledge for decision-making</span></li>
                <li style="margin-bottom: 6px; padding-left: 8px;"><strong style="color: var(--primary-color);">Hierarchical Task Planning:</strong> <span style="color: var(--text-color);">The system decomposes high-level goals into executable action sequences through strategic-to-tactical plan refinement, maintaining causal dependencies and temporal constraints</span></li>
                <li style="margin-bottom: 6px; padding-left: 8px;"><strong style="color: var(--primary-color);">Perception-Action Coupling:</strong> <span style="color: var(--text-color);">The system integrates symbolic reasoning outputs with sensorimotor control commands, translating cognitive decisions into parameterized robot behaviors through the dual-layer architecture</span></li>
              </ul>
             
             
              <p><strong>Ready to explore?</strong> Click "Next" or any step number above to begin!</p>
              
              <!-- Setup Images -->
              <div class="image-grid-container" style="margin: 30px 0; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                <!-- Simulation Setup Image -->
                <div class="image-container" style="text-align: center;">
                  <img src="assets/images/results/Sim_snippet.jpg" alt="Shipboard layout for simulations" class="clickable-image" style="width: 100%; height: 400px; object-fit: cover; border-radius: 8px; border: 1px solid var(--border-color); box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); cursor: pointer;" onclick="openImageModal('assets/images/results/Sim_snippet.jpg', 'Shipboard layout for simulations', 'Shipboard layout for simulations')">
                  <div class="image-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>Shipboard layout for simulations</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
                
                <!-- Robot Setup Image -->
                <div class="image-container" style="text-align: center;">
                  <img src="assets/images/results/Rob_Snippet.jpg" alt="Tabletop setup emulating the shipboard task" class="clickable-image" style="width: 100%; height: 400px; object-fit: cover; border-radius: 8px; border: 1px solid var(--border-color); box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); cursor: pointer;" onclick="openImageModal('assets/images/results/Rob_Snippet.jpg', 'Tabletop setup emulating the shipboard task', 'Tabletop setup emulating the shipboard task')">
                  <div class="image-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>Tabletop setup emulating the shipboard task</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
          <!-- Step 1 -->
          <div class="demo-step active" id="step1">
            <div class="step-title">
              <span class="step-number">1</span>
              Human Input: "It looks like the engine is overheating."
            </div>
            <div class="step-content">
              <div class="human-input">
                <strong>Human:</strong> "It looks like the engine is overheating."
              </div>
              <p><strong>What's happening:</strong> A human crew member reports a problem to the HARMONIC robot assistant. The system needs to understand not just the words, but the meaning and context behind them.</p>
              <p><strong>HARMONIC's interpretation:</strong> The system recognizes this as a mechanical problem description and creates a structured representation:</p>
              <pre class="code-block"><code>#DESCRIBE-MECHANICAL-PROBLEM.1
    agent               #HUMAN.1            // The speaker is describing
    beneficiary         #LEIA.1             // to the robot
    theme               #OVERHEAT.1         // the overheating.

#OVERHEAT.1
    theme               #ENGINE.1           // The engine is what is overheating.

#ENGINE.1
    corefer             ->ENGINE.1          // It is understood as the specific engine in the room.

#MODALITY.1
    attributed-to       #HUMAN.1            // The speaker
    type                BELIEF              // believes
    value               0.8                 // with relative certainty
    scope               #ASPECT.1           // that

#ASPECT.1
    scope               #OVERHEAT.1         // the state of overheating
    phase               CONTINUE            // is happening now.</code></pre>
            </div>
          </div>
          
          <!-- Step 2 -->
          <div class="demo-step" id="step2">
            <div class="step-title">
              <span class="step-number">2</span>
              Adjacency Pair Activation
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> HARMONIC's knowledge base contains "conversation patterns" that help it understand what typically happens after certain types of statements. This is like knowing that after someone says "I have a problem," you usually ask "What's wrong?" or offer help.</p>
              <p><strong>HARMONIC's reasoning:</strong> The system recognizes this as a problem description and knows that the appropriate response is to investigate potential causes. This is defined in the ontology as an "adjacency pair" - a conversation pattern:</p>
              <pre class="code-block"><code>@DESCRIBE-MECHANICAL-PROBLEM
    agent                   /HUMAN.1
    beneficiary             /LEIA.1
    theme                   /MECHANICAL-PROBLEM.1
    adjacency-pair-next     /HYPOTHESIZE-MECHANICAL-PROBLEM-CAUSE.1

/HYPOTHESIZE-MECHANICAL-PROBLEM-CAUSE.1
    agent                   /LEIA.1                 // The hypothesizer is the describee,
    beneficiary             /HUMAN.1                // the hypothesizee is the describer,
    theme                   /MECHANICAL-PROBLEM.1   // and the hypothesized cause is based on the described problem.</code></pre>
            </div>
          </div>
          
          <!-- Step 3 -->
          <div class="demo-step" id="step3">
            <div class="step-title">
              <span class="step-number">3</span>
              Plan Generation
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> Based on the conversation pattern, HARMONIC creates a specific plan to respond appropriately. Think of this like a human deciding "I should help diagnose this problem" after hearing about an issue.</p>
              <p><strong>HARMONIC's action:</strong> The system creates a concrete plan and adds it to its task list (agenda):</p>
              <pre class="code-block"><code>Plan.1
    #HYPOTHESIZE-MECHANICAL-PROBLEM-CAUSE.1
        agent               #LEIA.1                             // The agent will respond
        beneficiary         #HUMAN.1                            // to the speaker,
        theme               #OVERHEAT.1                         // about the engine's temperature.</code></pre>
            </div>
          </div>
          
          <!-- Step 4 -->
          <div class="demo-step" id="step4">
            <div class="step-title">
              <span class="step-number">4</span>
              Action Procedure Definition
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> HARMONIC needs to know exactly what to do to fulfill this plan. This is like a human thinking "To help diagnose this, I need to search my knowledge for possible causes."</p>
              <p><strong>HARMONIC's procedure:</strong> The system defines a specific action to take - this is a simple, atomic action that can't be broken down further:</p>
              <pre class="code-block"><code>@HYPOTHESIZE-MECHANICAL-PROBLEM-CAUSE
    *take-this-action       "Search the ontology for potential causes of the described problem report."
                            // Requires a @SPEECH-EFFECTOR.</code></pre>
            </div>
          </div>
          
          <!-- Step 5 -->
          <div class="demo-step" id="step5">
            <div class="step-title">
              <span class="step-number">5</span>
              Plan Execution
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> HARMONIC is now executing its plan to help diagnose the overheating problem. This is like a human mechanic thinking through what could cause an engine to overheat.</p>
              <p><strong>HARMONIC's execution process:</strong></p>
              <p>a) <strong>Check prerequisites:</strong> The system verifies it can proceed (no missing information or blocked conditions)</p>
              <p>b) <strong>Execute the procedure:</strong> HARMONIC searches its knowledge base for relevant information</p>
              <p>c) <strong>Knowledge base search:</strong> The system finds relevant information about engines and overheating:</p>
              <pre class="code-block"><code>@ENGINE
    theme-of            /OVERHEAT.1             // The engine can overheat.
    has-object-as-part  /THERMOSTAT.1           // The engine has a thermostat.

/OVERHEAT.1
    caused-by           /OBSTRUCT.1             // The overheating can be caused be an obstruction.
    caused-by           /STATE-OF-REPAIR.1      // The overheating can be caused by poor conditions.

/OBSTRUCT.1
    theme               /PIPE.1                 // The obstruction is of a pipe.

/STATE-OF-REPAIR.1
    domain              /THERMOSTAT.1           // The thermostat
    range               &lt; 0.7                   // might be faulty.</code></pre>
              <p>d) <strong>Analysis result:</strong> HARMONIC identifies two possible causes: a clogged pipe or a faulty thermostat. This is like a mechanic saying "It could be the cooling system or the temperature control."</p>
            </div>
          </div>
          
          <!-- Step 6 -->
          <div class="demo-step" id="step6">
            <div class="step-title">
              <span class="step-number">6</span>
              Response Generation
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> HARMONIC now needs to communicate its findings to the human in a clear, structured way. The system creates a formal representation of what it wants to say.</p>
              <p><strong>HARMONIC's response construction:</strong> The system builds a structured representation (GMR) of its intended response:</p>
              <pre class="code-block"><code>#ALTERNATIVE.1                                  // It might be either of two options,
    domain              #MODALITY.1
    range               #MODALITY.2

#MODALITY.1                                     // that a pipe is obstructed,
    type                EPISTEMIC
    value               0.5
    scope               #OBSTRUCT.1

#MODALITY.2                                     // or the thermostat is broken.
    type                EPISTEMIC
    value               0.5
    scope               #STATE-OF-REPAIR.1

#OBSTRUCT.1
    theme               @PIPE

#STATE-OF-REPAIR.1
    domain              @THERMOSTAT
    range               &lt; 0.7</code></pre>
              <p><strong>Text generation:</strong> The structured representation is converted into natural language that the human can understand:</p>
              <div class="robot-response">
                <strong>LEIA:</strong> "The thermostat might be broken or a pipe might be clogged."
              </div>
              <p><strong>Task management:</strong> While generating the response, HARMONIC continues monitoring its task list. Since this is the only active task, it waits for completion before moving on.</p>
              <p><strong>Completion:</strong> Once the response is delivered, the task is marked as complete:</p>
              <div class="plan-status status-finished">Plan.1 is FINISHED (the task is complete).</div>
            </div>
          </div>
          
          <!-- Step 7 -->
          <div class="demo-step" id="step7">
            <div class="step-title">
              <span class="step-number">7</span>
              Follow-up Question
            </div>
            <div class="step-content">
              <div class="human-input">
                <strong>Human:</strong> "When did we last replace the thermostat?"
              </div>
              <p><strong>What's happening:</strong> The human is now asking for specific historical information to help with the diagnosis. This is like asking "When did we last service this part?" to help determine if it might be the cause.</p>
              <p><strong>HARMONIC's interpretation:</strong> The system recognizes this as a request for temporal (time-based) information and creates a structured understanding:</p>
              <pre class="code-block"><code>#REQUEST-INFO-WHEN.1
    agent                   #HUMAN.1            // The speaker asks
    beneficiary             #LEIA.1             // the agent
    theme                   #EXCHANGE.1         // when the replacement happened.

#TEAM.1
    corefer                 ->TEAM.1            // "we" is understood as the @TEAM that the human and LEIA are on.

#EXCHANGE.1
    agent                   #TEAM.1             // The replacement happened by the team (or someone on it).
    theme                   #THERMOSTAT.1       // The replacement was a thermostat,

#THERMOSTAT.1
    corefer                 ->THERMOSTAT.1      // which is understood as the thermostat on the engine in the room.</code></pre>
            </div>
          </div>
          
          <!-- Step 8 -->
          <div class="demo-step" id="step8">
            <div class="step-title">
              <span class="step-number">8</span>
              Adjacency Pair Activation
            </div>
            <div class="step-content">
              <p>The root frame is defined in the ontology with an "adjacency pair", which prompts the agent to consider what should happen next:</p>
              <pre class="code-block"><code>@REQUEST-INFO-WHEN
    agent                   /HUMAN.1
    beneficiary             /LEIA.1
    theme                   /EVENT.1
    adjacency-pair-next     /RESPOND-TO-REQUEST-INFO-WHEN.1

/RESPOND-TO-REQUEST-INFO-WHEN.1
    agent                   /LEIA.1                 // The responder is the askee,
    beneficiary             /HUMAN.1                // the asker is the respondee,
    theme                   /EVENT.1                // and the response is about the asked event.</code></pre>
            </div>
          </div>
          
          <!-- Step 9 -->
          <div class="demo-step" id="step9">
            <div class="step-title">
              <span class="step-number">9</span>
              Plan Generation
            </div>
            <div class="step-content">
              <p>The agent makes a new plan following the adjacency pair, and puts it on agenda:</p>
              <pre class="code-block"><code>Plan.2
    #RESPOND-TO-REQUEST-INFO-WHEN.1
        agent               #LEIA.1                             // The agent will respond
        beneficiary         #HUMAN.1                            // to the speaker,
        theme               #EXCHANGE.1                         // about the last replacement

    #EXCHANGE.1
        theme               #THERMOSTAT.1                       // of the thermostat
        agent               #TEAM.1                             // by the team.</code></pre>
            </div>
          </div>
          
          <!-- Step 10 -->
          <div class="demo-step" id="step10">
            <div class="step-title">
              <span class="step-number">10</span>
              Action Procedure Definition
            </div>
            <div class="step-content">
              <p>The plan represents an atomic (reflexive) action; it has no substeps that the agent can decompose, but it does have a *take-this-action procedure, defined (in gloss) as:</p>
              <pre class="code-block"><code>@RESPOND-TO-REQUEST-INFO-WHEN
    *take-this-action       "Lookup the database of maintenance logs, and report the answer."
                            // Requires a @SPEECH-EFFECTOR.</code></pre>
            </div>
          </div>
          
          <!-- Step 11 -->
          <div class="demo-step" id="step11">
            <div class="step-title">
              <span class="step-number">11</span>
              Plan Execution
            </div>
            <div class="step-content">
              <p>The agent activates the plan.</p>
              <p>a) The NEXT step selected is #RESPOND-TO-REQUEST-INFO-WHEN.1, which has no preconditions that are failing.</p>
              <p>b) The procedure is executed.</p>
              <p>c) The database of maintenance logs has the following relevant information:</p>
              <pre class="code-block"><code>Maintenance logs found:

replaced thermostat, 6/2/2014
oiled gears, 2/5/2017
oiled gears, 3/6/2018
replaced thermostat, 9/17/2019
oiled gears, 4/22/2020</code></pre>
              <p>d) The determined result is 9/17/2019.</p>
              <p>e) The agent builds the following GMR to represent its intended report:</p>
              <pre class="code-block"><code>#DESCRIBE-TIME.1
    agent               #LEIA.1
    beneficiary         #HUMAN.1
    absolute-time       9/17/2019</code></pre>
              <p>f) The GMR is sent to the text generation system, which produces:</p>
              <div class="robot-response">
                <strong>LEIA:</strong> "5 years ago."
              </div>
              <p>g) The agent continues to attend to its agenda during the time this takes:</p>
              <p>Plan.2 is RUNNING, so there is no effect (it must finish before continuing).</p>
              <p>h) The procedure finishes, which sends a callback:</p>
              <div class="plan-status status-finished">Plan.2 is FINISHED (the task is complete).</div>
            </div>
          </div>
          
          <!-- Step 12 -->
          <div class="demo-step" id="step12">
            <div class="step-title">
              <span class="step-number">12</span>
              Action Request
            </div>
            <div class="step-content">
              <div class="human-input">
                <strong>Human:</strong> "Ok. Bring me a new thermostat."
              </div>
              <p><strong>What's happening:</strong> Based on the diagnosis, the human has decided to replace the thermostat and is now asking HARMONIC to physically retrieve a new one. This shifts from information exchange to physical action.</p>
              <p><strong>HARMONIC's interpretation:</strong> The system recognizes this as a request for physical action (fetching an object) and creates a structured understanding:</p>
              <pre class="code-block"><code>#REQUEST-ACTION-FETCH.1
    agent                   #HUMAN.1            // The speaker is asking
    beneficiary             #LEIA.1             // the listener to fetch
    theme                   #THERMOSTAT.1       // a thermostat

#THERMOSTAT.1
    age                     0.0001 <> 0.1       // that is new.
                                                    // Importantly, there is no corefer - which thermostat is not known.</code></pre>
            </div>
          </div>
          
          <!-- Step 13 -->
          <div class="demo-step" id="step13">
            <div class="step-title">
              <span class="step-number">13</span>
              Adjacency Pair Activation
            </div>
            <div class="step-content">
              <p>The root frame is defined in the ontology with an "adjacency pair", which prompts the agent to consider what should happen next:</p>
              <pre class="code-block"><code>@REQUEST-ACTION-FETCH
    agent                   /HUMAN.1
    beneficiary             /LEIA.1
    theme                   /PHYSICAL-OBJECT.1
    adjacency-pair-next     /FETCH.1

/FETCH.1
    agent                   /LEIA.1                 // The askee is the fetcher,
    beneficiary             /HUMAN.1                // the asker is the fetchee,
    theme                   /PHYSICAL-OBJECT.1      // and the object being asked for is what is returned.</code></pre>
            </div>
          </div>
          
          <!-- Step 14 -->
          <div class="demo-step" id="step14">
            <div class="step-title">
              <span class="step-number">14</span>
              Plan Generation
            </div>
            <div class="step-content">
              <p>The agent makes a new plan following the adjacency pair, and puts it on agenda:</p>
              <pre class="code-block"><code>Plan.3
    #FETCH.1
        agent               #LEIA.1                             // The agent will fetch
        beneficiary         #HUMAN.1                            // for the speaker,
        theme               #THERMOSTAT.1                       // a thermostat that is new.
        precondition        #THERMOSTAT.features NOT NULL       // Any discerning features must be known.

    #THERMOSTAT.1
        age                 0.0001 <> 0.1</code></pre>
            </div>
          </div>
          
          <!-- Step 15 -->
          <div class="demo-step" id="step15">
            <div class="step-title">
              <span class="step-number">15</span>
              Multi-Step Plan Definition
            </div>
            <div class="step-content">
              <p>The plan represents a multi-step action, the entirety of which is moved into the plan (not shown):</p>
              <pre class="code-block"><code>@FETCH
    agent                   /LEIA.1
    beneficiary             /HUMAN.1
    theme                   /PHYSICAL-OBJECT.1

    has-event-as-part       /BACKCHANNEL.1                      // Confirm (if this was from a request).
    has-event-as-part       /SEARCH.1                           // Search using the embodied search algorithm.
    has-event-as-part       /WAIT-EVENT.1                       // Keep searching until the object is found.
    has-event-as-part       /PICK-UP.1                          // Pick up the object.
    has-event-as-part       /NAVIGATE-TO.1                      // Navigate to the beneficiary.
    has-event-as-part       /GIVE.1                             // Give the object to the beneficiary.

    starts                  /BACKCHANNEL.1

/BACKCHANNEL.1
    agent                   /LEIA.1
    beneficiary             /HUMAN.1
    meets                   /SEARCH.1

/SEARCH.1
    agent                   /LEIA.1
    theme                   /PHYSICAL-OBJECT.1
    meets                   /WAIT-EVENT.1

/WAIT-EVENT.1
    agent                   /LEIA.1
    theme                   /PHYSICAL-OBJECT.1.absolute-location NOT NULL
    meets                   /PICK-UP.1

/PICK-UP.1
    agent                   /LEIA.1
    theme                   /PHYSICAL-OBJECT.1
    meets                   /NAVIGATE-TO.1

/NAVIGATE-TO.1
    agent                   /LEIA.1
    theme                   /HUMAN.1
    meets                   /GIVE.1

/GIVE.1
    agent                   /LEIA.1
    theme                   /PHYSICAL-OBJECT.1
    beneficiary             /HUMAN.1</code></pre>
            </div>
          </div>
          
          <!-- Step 16 -->
          <div class="demo-step" id="step16">
            <div class="step-title">
              <span class="step-number">16</span>
              Precondition Check
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> HARMONIC tries to start the fetch task but realizes it doesn't have enough information to proceed. This is like a human saying "I need to know what the thermostat looks like before I can find it."</p>
              <p><strong>HARMONIC's problem detection:</strong> The system checks if it has all the information needed to complete the task. It discovers that while it knows it needs to find a "thermostat," it doesn't know what specific thermostat to look for or how to identify it:</p>
              <pre class="code-block"><code>Precondition Check:
#THERMOSTAT.features NOT NULL
❌ FAILED - Features not known</code></pre>
            </div>
          </div>
          
          <!-- Step 17 -->
          <div class="demo-step" id="step17">
            <div class="step-title">
              <span class="step-number">17</span>
              Information Request Plan
            </div>
            <div class="step-content">
              <p>The agent looks in its ontology for a plan that can satisfy the precondition. It finds the following:</p>
              <pre class="code-block"><code>@REQUEST-INFO-VISUAL-DETAILS
    agent                   /LEIA.1
    beneficiary             /HUMAN.1
    theme                   @PHYSICAL-OBJECT
    effect                  $.theme.features NOT NULL       // After this event, the features of the object will be known.
    has-event-as-part       /RENDER-GMR.1
    has-event-as-part       /WAIT-EVENT.1
    starts                  /RENDER-GMR.1

/RENDER-GMR.1                                               // First the agent asks the question.
    agent                   /LEIA.1
    beneficiary             /HUMAN.1
    meets                   /WAIT-EVENT.1

/WAIT-EVENT.1
    agent                   /LEIA.1                         // Then the agent awaits a response.
    theme                   $.theme.features NOT NULL</code></pre>
            </div>
          </div>
          
          <!-- Step 18 -->
          <div class="demo-step" id="step18">
            <div class="step-title">
              <span class="step-number">18</span>
              Sub-Plan Creation
            </div>
            <div class="step-content">
              <p>The agent makes a plan to satisfy the precondition:</p>
              <pre class="code-block"><code>Plan.4
    #REQUEST-INFO-VISUAL-DETAILS.1
        agent               #LEIA.1
        beneficiary         #HUMAN.1
        theme               #THERMOSTAT.1
        has-event-as-part   #RENDER-GMR.1
        has-event-as-part   #WAIT-EVENT.1
        starts              #RENDER-GMR.1

    #RENDER-GMR.1
        agent               #LEIA.1
        beneficiary         #HUMAN.1
        meets               #WAIT-EVENT.1

    #WAIT-EVENT.1
        agent               #LEIA.1
        theme               #THERMOSTAT.1.features NOT NULL</code></pre>
            </div>
          </div>
          
          <!-- Step 19 -->
          <div class="demo-step" id="step19">
            <div class="step-title">
              <span class="step-number">19</span>
              Plan Activation
            </div>
            <div class="step-content">
              <p>The agent activates the plan.</p>
              <p>a) The NEXT step selected is #RENDER-GMR.1, which has no preconditions that are failing.</p>
              <p>b) The procedure is executed.</p>
              <p>c) The GMR is sent to the text generation system, which produces:</p>
              <div class="robot-response">
                <strong>LEIA:</strong> "What does it look like?"
              </div>
              <p>d) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-impassed">Plan.3 is IMPASSED, so there is no effect (it must finish before continuing).</div>
              <div class="plan-status status-running">Plan.4 is RUNNING, so there is no effect (it must finish before continuing).</div>
              <p>h) The procedure finishes, which sends a callback:</p>
              <div class="plan-status status-impassed">Plan.3 is IMPASSED, so there is no effect (it must finish before continuing).</div>
              <div class="plan-status status-pending">Plan.4 is PENDING (the step is complete, but the plan is not).</div>
            </div>
          </div>
          
          <!-- Step 20 -->
          <div class="demo-step" id="step20">
            <div class="step-title">
              <span class="step-number">20</span>
              Plan Activation (Continued)
            </div>
            <div class="step-content">
              <p>The agent activates the plan (again).</p>
              <p>a) The NEXT step selected is #WAIT-EVENT.1, which has no preconditions that are failing.</p>
              <p>b) The procedure is executed.</p>
              <p>c) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-impassed">Plan.3 is IMPASSED, so there is no effect (it must finish before continuing).</div>
              <div class="plan-status status-running">Plan.4 is RUNNING, so there is no effect (it must finish before continuing).</div>
            </div>
          </div>
          
          <!-- Step 21 -->
          <div class="demo-step" id="step21">
            <div class="step-title">
              <span class="step-number">21</span>
              Human Response
            </div>
            <div class="step-content">
              <div class="human-input">
                <strong>Human:</strong> "It is labeled thermostat."
              </div>
              <p><strong>What's happening:</strong> The human provides the missing information that HARMONIC needs to identify the correct object. This is like telling someone "Look for the box that says 'thermostat' on it."</p>
              <p><strong>HARMONIC's information update:</strong> The system now updates its knowledge with the visual details needed to identify the object:</p>
              <pre class="code-block"><code>#DESCRIBE-INFO-VISUAL-DETAILS.1
    agent                   #HUMAN.1
    beneficiary             #LEIA.1
    theme                   #THERMOSTAT.1       // Visual details of the thermostat are

#THERMOSTAT.1
    has-label               "thermostat"        // that is has a label "thermostat" on it.</code></pre>
            </div>
          </div>
          
          <!-- Step 22 -->
          <div class="demo-step" id="step22">
            <div class="step-title">
              <span class="step-number">22</span>
              Plan Status Update
            </div>
            <div class="step-content">
              <p>The agent activates the plan (again).</p>
              <p>a) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-pending">Plan.3 is PENDING, as the precondition is now satisfied.</div>
              <div class="plan-status status-finished">Plan.4 is FINISHED, as the wait condition is now complete.</div>
              <p>b) The NEXT step selected is #BACKCHANNEL.1, which has no preconditions that are failing.</p>
              <p>c) The procedure is executed.</p>
              <p>d) The agent generates the following text:</p>
              <div class="robot-response">
                <strong>LEIA:</strong> "On it!"
              </div>
              <p>e) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-running">Plan.3 is RUNNING, so there is no effect (it must finish before continuing).</div>
              <p>f) The procedure finishes, which sends a callback:</p>
              <div class="plan-status status-pending">Plan.3 is PENDING (the step is complete, but the plan is not).</div>
            </div>
          </div>
          
          <!-- Step 23 -->
          <div class="demo-step" id="step23">
            <div class="step-title">
              <span class="step-number">23</span>
              Search Initiation
            </div>
            <div class="step-content">
              <p>The agent activates the plan (again).</p>
              <p style="margin: 5px 0;">a) The NEXT step selected is #SEARCH.1, which has no preconditions that are failing.</p>
              <p style="margin: 5px 0;">b) The procedure is executed.</p>
              <p style="margin: 5px 0;">c) The agent begins the embodied search heuristic.</p>
              
              <!-- Videos for search demonstration -->
              <div class="video-grid-container" style="margin: 30px 0; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                <!-- Search Simulation Video -->
                <div class="video-container" style="text-align: center;">
                  <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; object-position: top; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Search_Sim.mp4', 'UGV searching for the thermostat in a ship simulation.')">
                    <source src="assets/videos/demos/Search_Sim.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>UGV searching for the thermostat in a ship simulation.</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
                
                <!-- Search Robot Video -->
                <div class="video-container" style="text-align: center;">
                  <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Search_Robot.mp4', 'Tabletop robot searching for the thermostat on the shelf.')">
                    <source src="assets/videos/demos/Search_Robot.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>Tabletop robot searching for the thermostat on the shelf.</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
              </div>
              
              <p>d) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-running">Plan.3 is RUNNING, so there is no effect (it must finish before continuing).</div>
              <p>e) The procedure finishes, which sends a callback:</p>
              <div class="plan-status status-pending">Plan.3 is PENDING (the step is complete, but the plan is not).</div>
              <p>This procedure is specifically asynchronous - the search continues, but the plan moves on.</p>
            </div>
          </div>
          
          <!-- Step 24 -->
          <div class="demo-step" id="step24">
            <div class="step-title">
              <span class="step-number">24</span>
              Wait for Object Location
            </div>
            <div class="step-content">
              <p>The agent activates the plan (again).</p>
              <p>a) The NEXT step selected is #WAIT-EVENT.1, which has no preconditions that are failing.</p>
              <p>b) The procedure is executed.</p>
              <p>c) The agent now waits for the location of the thermostat to be known.</p>
              <p>d) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-running">Plan.3 is RUNNING, so there is no effect (it must finish before continuing).</div>
            </div>
          </div>
          
          <!-- Step 25 -->
          <div class="demo-step" id="step25">
            <div class="step-title">
              <span class="step-number">25</span>
              Object Detection
            </div>
            <div class="step-content">
              <p>New visual data is observed, resulting in the following VMR:</p>
              <pre class="code-block"><code>#VOLUNTARY-VISUAL-EVENT.1
                agent                   #LEIA.1
                theme                   #THERMOSTAT.1
                    
            #THERMOSTAT.1
                has-label               "thermostat"
                absolute-location       37.5, 19.2, 86.3
                on-top-of               ->SHELF.1</code></pre>
              
              <!-- Images for object detection -->
              <div class="image-grid-container" style="margin: 20px 0; display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px; align-items: start;">
                <!-- Simulation image -->
                <div class="image-container" style="text-align: center;">
                  <img src="assets/images/results/thermostat_found_sim.jpg" alt="Object Detection Simulation" class="clickable-image" style="width: 100%; border-radius: 4px; border: 1px solid var(--border-color); box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); cursor: pointer;" onclick="openImageModal('assets/images/results/thermostat_found_sim.jpg', 'Object Detection Simulation', 'Simulation view showing object detection results')">
                  <div class="figure-caption">
                    <p><strong>Simulation View</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
                
                <!-- Robot View 1 -->
                <div class="image-container" style="text-align: center;">
                  <img src="assets/images/results/thermostat_found_robot.jpg" alt="Robot View 1" class="clickable-image" style="width: 100%; border-radius: 4px; border: 1px solid var(--border-color); box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); cursor: pointer;" onclick="openImageModal('assets/images/results/thermostat_found_robot.jpg', 'Robot View 1', 'Robot perspective showing object detection')">
                  <div class="figure-caption">
                    <p><strong>Robot View 1</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
                
                <!-- Robot View 2 -->
                <div class="image-container" style="text-align: center;">
                  <img src="assets/images/results/thermostat_found_robot_cam.jpg" alt="Robot View 2" class="clickable-image" style="width: 100%; border-radius: 4px; border: 1px solid var(--border-color); box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); cursor: pointer;" onclick="openImageModal('assets/images/results/thermostat_found_robot_cam.jpg', 'Robot View 2', 'Robot camera view showing object detection')">
                  <div class="figure-caption">
                    <p><strong>Robot View 2</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
              </div>
              
              
            </div>
          </div>
          
          <!-- Step 26 -->
          <div class="demo-step" id="step26">
            <div class="step-title">
              <span class="step-number">26</span>
              Object Pickup
            </div>
            <div class="step-content">
              <p>The agent activates the plan (again).</p>
              <p style="margin: 5px 0;">a) The awaited condition by #WAIT-EVENT.1 is now satisfied, so:</p>
              <div class="plan-status status-pending">Plan.3 is PENDING (the step is complete, but the plan is not).</div>
              <p style="margin: 5px 0;">b) The NEXT step selected is #PICK-UP.1, which has no preconditions that are failing.</p>
              <p style="margin: 5px 0;">c) The procedure is executed.</p>
              <p style="margin: 5px 0;">d) The agent continues to attend to its agenda during the time this takes:</p>
              <div class="plan-status status-running">Plan.3 is RUNNING, so there is no effect (it must finish before continuing).</div>
              <p style="margin: 5px 0;">e) The procedure finishes, which sends a callback:</p>
              <div class="plan-status status-pending">Plan.3 is PENDING (the step is complete, but the plan is not).</div>
              
              <div style="background: linear-gradient(135deg, rgba(255, 193, 7, 0.1) 0%, rgba(255, 152, 0, 0.1) 100%); border-left: 4px solid #ff9800; padding: 15px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(255, 152, 0, 0.1);">
                <p style="margin: 0; font-weight: 600; color: #008cff; font-size: 16px; line-height: 1.6;">
                  <strong>⚠️ Important:</strong>While the agent maintains a waiting state, this does not indicate system idleness; rather, the robotic system continues active sensing operations and action execution, with the cognitive processing module awaiting task-relevant perceptual feedback to proceed with plan advancement.
                </p>
              </div>
              
              <!-- Videos for pickup demonstration -->
              <div class="video-grid-container" style="margin: 30px 0; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                <!-- Pickup Simulation Video -->
                <div class="video-container" style="text-align: center;">
                  <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; object-position: bottom; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Pickup_Sim.mp4', 'UGV executing pickup sequence in simulation')">
                    <source src="assets/videos/demos/Pickup_Sim.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>UGV executing pickup sequence in simulation</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
                
                <!-- Pickup Robot Video -->
                <div class="video-container" style="text-align: center;">
                  <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Pickup_Robot.mp4', 'Tabletop robot executing the pickup sequence')">
                    <source src="assets/videos/demos/Pickup_Robot.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>Tabletop robot executing the pickup sequence</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
          <!-- Step 27 -->
          <div class="demo-step" id="step27">
            <div class="step-title">
              <span class="step-number">27</span>
              Navigation
            </div>
              <div class="step-content">
                <p>The agent activates the plan (again).</p>
                <p style="margin: 5px 0;">a) The NEXT step selected is #NAVIGATE-TO.1, which has no preconditions that are failing.</p>
                <p style="margin: 5px 0;">b) The procedure is executed.</p>
                <p style="margin: 5px 0;">c) The agent continues to attend to its agenda during the time this takes:</p>
                <div class="plan-status status-running">Plan.3 is RUNNING, so there is no effect (it must finish before continuing).</div>
                <p style="margin: 5px 0;">e) The procedure finishes, which sends a callback:</p>
                <div class="plan-status status-pending">Plan.3 is PENDING (the step is complete, but the plan is not).</div>
                
                <!-- Videos for return demonstration -->
                <div class="video-grid-container" style="margin: 30px 0; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                  <!-- Return Simulation Video -->
                  <div class="video-container" style="text-align: center;">
                    <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; object-position: top; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Return_Sim.mp4', 'UGV returning thermostat to Danny in Simulation')">
                      <source src="assets/videos/demos/Return_Sim.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                      <p><strong>UGV returning thermostat to Danny in Simulation</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                    </div>
                  </div>
                  
                  <!-- Return Robot Video -->
                  <div class="video-container" style="text-align: center;">
                    <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Return_Robot.mp4', 'Tabletop robot returning thermostat to drop area')">
                      <source src="assets/videos/demos/Return_Robot.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                      <p><strong>Tabletop robot returning thermostat to drop area</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                    </div>
                  </div>
                </div>
              </div>
          </div>
          
          <!-- Step 28 -->
          <div class="demo-step" id="step28">
            <div class="step-title">
              <span class="step-number">28</span>
              Delivery
            </div>
            <div class="step-content">
              <p>The agent activates the plan (again).</p>
              <p style="margin: 5px 0;">a) The NEXT step selected is #GIVE.1, which has no preconditions that are failing.</p>
              <p style="margin: 5px 0;">b) @GIVE is a multi-step event (not shown) that causes the agent to:</p>
              <ul>
                <li>Say "Here you go." while simultaneously,</li>
                <li>offer the thermostat to the speaker.</li>
              </ul>
              <div class="robot-response">
                <strong>LEIA:</strong> "Here you go."
              </div>
              <p><strong>Task completion:</strong> The entire fetch task is now complete:</p>
              <div class="plan-status status-finished">Plan.3 FINISHED</div>
              <p><strong>Summary:</strong> HARMONIC has successfully completed the entire interaction cycle: understanding the problem, diagnosing potential causes, providing historical information, and physically retrieving the requested object. This demonstrates the system's ability to seamlessly integrate natural language understanding, reasoning, and physical action.</p>
              
              <!-- Videos for drop demonstration -->
              <div class="video-grid-container" style="margin: 30px 0; display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: start;">
                <!-- Drop Simulation Video -->
                <div class="video-container" style="text-align: center;">
                  <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; object-position: bottom; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Drop_Sim.mp4', 'UGV drops thermostat after return')">
                    <source src="assets/videos/demos/Drop_Sim.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>UGV drops thermostat after return</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
                
                <!-- Drop Robot Video -->
                <div class="video-container" style="text-align: center;">
                  <video class="clickable-video" autoplay muted loop playsinline style="width: 100%; height: 300px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/Drop_Robot.mp4', 'Tabletop robot drops thermostat in drop area')">
                    <source src="assets/videos/demos/Drop_Robot.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption" style="font-size: 14px; color: var(--text-color);">
                    <p><strong>Tabletop robot drops thermostat in drop area</strong> <em style="color: var(--accent-color); font-size: 0.9em;">(Click to zoom)</em></p>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
          <!-- Step 29 -->
          <div class="demo-step" id="step29">
            <div class="step-title">
              <span class="step-number">29</span>
              Watch Full Demo - Simulation
            </div>
            <div class="step-content">
              <p><strong>Complete HARMONIC Simulation Demonstration</strong></p>
              <p>Watch the full simulation showing HARMONIC's complete reasoning and execution process in a simulated environment.</p>
              
              <!-- Full-width simulation video -->
              <div class="full-width-video-container" style="margin: 30px 0; width: 100%;">
                <video controls muted autoplay loop class="full-demo-video" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);">
                  <source src="assets/videos/demos/demo1_simulation.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <div class="figure-caption">
                  <p><strong>Complete HARMONIC Simulation Demo</strong></p>
                </div>
              </div>
              
            </div>
          </div>
          
          <!-- Step 30 -->
          <div class="demo-step" id="step30">
            <div class="step-title">
              <span class="step-number">30</span>
              Watch Full Demo - Real-Robot
            </div>
            <div class="step-content">
              <p><strong>Complete HARMONIC Real-Robot Demonstration</strong></p>
              <p>Watch the full real-robot demonstration showing HARMONIC's complete reasoning and execution process with physical hardware.</p>
              
              <!-- Full-width robot video -->
              <div class="full-width-video-container" style="margin: 30px 0; width: 100%;">
                <video controls muted autoplay loop class="full-demo-video" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);">
                  <source src="assets/videos/demos/demo2_robot.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <div class="figure-caption">
                  <p><strong>Complete HARMONIC Real-Robot Demo</strong></p>
                </div>
              </div>
              
            </div>
          </div>
        </div>
      </div>
      </section>
      
      <hr class="section-divider">

      <!-- Evaluation 2 Section -->
      <section id="evaluation2">
      <h2>Evaluation - 2 (Multi-Robot Search and Retrieve)</h2>
      <div class="interactive-demo">
        <div class="demo-header">
          <div class="demo-title">Step-by-Step HARMONIC Distributed Planning Process</div>
          <div class="demo-description">
            This interactive demonstration shows how HARMONIC distributes plans between strategic and tactical components across multiple robots (UGV and drone). <strong>Click any step number above to jump directly to that part of the process</strong>, or use the Previous/Next buttons to go through sequentially. Each step shows the detailed collaborative planning and execution behind HARMONIC's multi-robot coordination.
          </div>
          <div class="scenario-description" style="margin-top: 25px; padding: 20px; background: linear-gradient(135deg, rgba(52, 152, 219, 0.08) 0%, rgba(155, 89, 182, 0.08) 100%); border-radius: 12px; border: 1px solid rgba(52, 152, 219, 0.2); box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05); position: relative; overflow: hidden;">
            <div style="position: absolute; top: 0; left: 0; width: 4px; height: 100%; background: linear-gradient(180deg, var(--accent-color) 0%, #9b59b6 100%);"></div>
            <div style="position: relative; z-index: 1;">
              <div style="display: flex; align-items: center; margin-bottom: 12px;">
                <div style="width: 12px; height: 12px; background: linear-gradient(135deg, var(--accent-color) 0%, #9b59b6 100%); border-radius: 3px; margin-right: 12px; transform: rotate(45deg); box-shadow: 0 2px 6px rgba(52, 152, 219, 0.3);"></div>
                <h4 style="margin: 0; color: var(--accent-color); font-size: 16px; font-weight: 600; letter-spacing: 0.5px;">SCENARIO 2</h4>
              </div>
              <p style="margin: 0; font-style: italic; color: var(--text-color); line-height: 1.7; font-size: 15px; text-align: justify;">
                In an apartment environment, a heterogeneous multi-robot team consisting of a <strong style="color: var(--accent-color); font-style: normal;">UGV</strong> and a <strong style="color: var(--accent-color); font-style: normal;">drone</strong> assists a human, <strong style="color: var(--accent-color); font-style: normal;">Danny</strong>, in locating a lost set of keys.
              </p>
            </div>
          </div>
        </div>
        
        <div class="demo-controls">
          <button class="demo-button" id="prevStep2" onclick="previousStep2()">← Previous</button>
          <button class="demo-button" id="nextStep2" onclick="nextStep2()">Next →</button>
          <button class="demo-button" id="autoPlayBtn2" onclick="toggleAutoPlay2()">Auto Play</button>
          <button class="demo-button simulation-button" onclick="goToStep2(10)" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none;">
            Watch Multi-Robot Demo
          </button>
          <div class="step-counter">
            Step <span id="currentStep2">0</span> of <span id="totalSteps2">10</span>
          </div>
        </div>
        
        <!-- Step Navigation Menu -->
        <div class="step-navigation">
          <div class="step-nav-title">Jump to Step:</div>
          <div class="step-nav-grid" id="stepNavGrid2">
            <!-- Step navigation buttons will be generated by JavaScript -->
          </div>
        </div>
        
        <div class="demo-content" id="demoContent2">
          <!-- Introduction -->
          <div class="demo-step active" id="step2-0" style="display: none;">
            <div class="step-title">
              <span class="step-number">0</span>
              Understanding HARMONIC's Distributed Planning Process
            </div>
            <div class="step-content">
              <p><strong>Welcome to the HARMONIC Multi-Robot Demo!</strong></p>
              <p>This demonstration shows how HARMONIC coordinates multiple robots through distributed planning, where high-level strategic plans are divided into tactical commands for collaborative execution.</p>
              
              <h4>What You'll See:</h4>
              <ul>
                <li><strong>Collaborative Activity Scripts:</strong> How team leaders and subordinates coordinate through meta-scripts</li>
                <li><strong>Distributed Plan Execution:</strong> How plans are divided between strategic and tactical layers</li>
                <li><strong>Multi-Robot Coordination:</strong> How UGV and drone work together in search missions</li>
                <li><strong>Behavior Tree Integration:</strong> How tactical layers execute collaborative plans</li>
                <li><strong>Real-time Communication:</strong> How robots coordinate and report findings</li>
              </ul>
              
              <p><strong>Ready to explore?</strong> Click "Next" or any step number above to begin!</p>
            </div>
          </div>
          
          <!-- Step 1 -->
          <div class="demo-step active" id="step2-1">
            <div class="step-title">
              <span class="step-number">1</span>
              Task Initiation: Danny's Request (M1)
            </div>
            <div class="step-content">
              <div class="human-input">
                <strong>DANNY:</strong> "I think I left my keys at home. Can you look around for them?"
              </div>
              <p><strong>What's happening:</strong> Danny initiates the search task by sending a message to the robot team. This triggers the team leader (UGV-U) to place a COLLABORATIVE-ACTIVITY on its agenda.</p>
              <p><strong>HARMONIC's response:</strong> The UGV-U recognizes this as a search task and begins the collaborative planning process:</p>
              <pre class="code-block"><code>@COLLABORATIVE-ACTIVITY (leader)
[INIT]
  *identify-team-members
[SELECT-PLAN]
  // Select a plan from available options.
  RUN *identify-candidate-plans
  RUN *select-plan
  // Selected: SEARCH-FOR-LOST-OBJECT</code></pre>
            </div>
          </div>
          
          <!-- Step 2 -->
          <div class="demo-step" id="step2-2">
            <div class="step-title">
              <span class="step-number">2</span>
              Information Gathering: Object Description (M2-M3)
            </div>
            <div class="step-content">
              <div class="robot-output">
                <strong>UGV-U:</strong> "What do they look like?"
              </div>
              <div class="human-input">
                <strong>DANNY:</strong> "They are on a red keychain with a small flashlight."
              </div>
              <p><strong>What's happening:</strong> The UGV-U verifies preconditions for the SEARCH-FOR-LOST-OBJECT plan by gathering information about the object's appearance.</p>
              <p><strong>Strategic layer processing:</strong> The UGV-U updates its plan with the new information:</p>
              <pre class="code-block"><code>[PRECONDITIONS]
  // Preconditions from the selected plan
  RUN NEW @REQUEST-OBJECT-TYPE ✓
  RUN NEW @REQUEST-OBJECT-FEATURES ✓
  // Features: red keychain, small flashlight
  RUN NEW @REQUEST-LAST-SEEN-AT
  RUN NEW @REQUEST-LOCATION-CONSTRAINED</code></pre>
            </div>
          </div>
          
          <!-- Step 3 -->
          <div class="demo-step" id="step2-3">
            <div class="step-title">
              <span class="step-number">3</span>
              Location Information: Last Known Location (M4-M5)
            </div>
            <div class="step-content">
              <div class="robot-output">
                <strong>UGV-U:</strong> "Do you recall where you last had them?"
              </div>
              <div class="human-input">
                <strong>DANNY:</strong> "I used them last night to open the front door, but they could be anywhere."
              </div>
              <p><strong>What's happening:</strong> The UGV-U gathers location information to prioritize search areas. The front door area becomes a priority location for the search sequence.</p>
              <p><strong>Plan instantiation:</strong> The UGV-U now has enough information to instantiate the complete search plan:</p>
              <pre class="code-block"><code>@SEARCH-FOR-LOST-OBJECT
  [SEARCH-ZONES]
    // Search in each ZONE the agent knows.
    // Priority: FRONT DOOR (last used location)
    FOR #ZONE-1 IN #LOCATION-1.SEARCHABLE-ZONE 
      RUN ASYNC AWAIT *search 
      INTERRUPT WHEN #OBJECT-1.LOCATION KNOWN
      RUN *consider-reporting</code></pre>
            </div>
          </div>
          
          <!-- Step 4 -->
          <div class="demo-step" id="step2-4">
            <div class="step-title">
              <span class="step-number">4</span>
              Plan Distribution: Search Initiation (M6)
            </div>
            <div class="step-content">
              <div class="robot-output">
                <strong>UGV-U:</strong> "Let's search the apartment."
              </div>
              <p><strong>What's happening:</strong> With preconditions met, the UGV-U moves to SUGGEST-PLAN and shares the domain plan with the drone through natural language dialog.</p>
              <p><strong>Subordinate response:</strong> The DRONE-D receives this information and adopts its assignment within the suggested plan:</p>
              <pre class="code-block"><code>@COLLABORATIVE-ACTIVITY (subordinate)
[INIT]
  *identify-team-members
[WAIT-FOR-PLAN]
  // Wait for a plan from the leader
  AWAIT $.HAS-COLLABORATIVE-PLAN ISA @EVENT ✓
[RUN-PLAN]
  // Execute assigned search area: APARTMENT
  RUN NEW @SEARCH-FOR-LOST-OBJECT</code></pre>
            </div>
          </div>
          
          
          <!-- Step 5 -->
          <div class="demo-step" id="step2-5">
            <div class="step-title">
              <span class="step-number">5</span>
              Search Execution: Coordinated Exploration
            </div>
            <div class="step-content">
              <p><strong>What's happening:</strong> Both robots begin exploring their assigned areas using a waypoint strategy controlled by their individual tactical modules.</p>
              <p><strong>Strategic vs Tactical:</strong> The strategic (cognitive) module maintains awareness of area existence without directly guiding robot navigation, allowing for efficient local path planning while preserving high-level planning.</p>
     
            </div>
          </div>
          
          <!-- Step 6 -->
          <div class="demo-step" id="step2-6">
            <div class="step-title">
              <span class="step-number">6</span>
              Communication and Coordination (M7)
            </div>
            <div class="step-content">
              <div class="robot-output">
                <strong>UGV-U:</strong> "They aren't in the doorway."
              </div>
              <p><strong>What's happening:</strong> Throughout the search, robots report their findings to each other. When a robot fails to locate the keys in a searched area, it communicates this to its partner.</p>
              <p><strong>Coordination protocol:</strong> This continuous communication ensures efficient coverage and prevents redundant searching. The UGV-U reports that the doorway area has been searched without success.</p>
            </div>
          </div>
          
          <!-- Step 7 -->
          <div class="demo-step" id="step2-7">
            <div class="step-title">
              <span class="step-number">7</span>
              Object Detection: Keys Found
            </div>
            <div class="step-content">
              <div class="robot-output">
                <strong>UGV-U:</strong> "I found them north of the couch."
              </div>
              <p><strong>What's happening:</strong> The UGV-U's Vision Meaning Representations (VMRs) widget displays object detection results that the strategic layer processes from sensing frames communicated by the tactical layer.</p>
              <p><strong>Strategic analysis:</strong> The strategic module continuously analyzes sensor data frames, grounding these VMRs against the instance of the KEY object stored in its episodic memory. The keys are located north of the couch.</p>
            </div>
          </div>
          
          <!-- Step 8 -->
          <div class="demo-step" id="step2-8">
            <div class="step-title">
              <span class="step-number">8</span>
              Task Completion: Mission Report
            </div>
            <div class="step-content">
              <div class="robot-output">
                <strong>UGV-U:</strong> "I found them behind the couch."
              </div>
              <p><strong>What's happening:</strong> When the keys are found, the UGV-U reports the successful completion to Danny. Notice the difference in location description: "north of the couch" (M8) vs "behind the couch" (M9).</p>
              <p><strong>Context-appropriate communication:</strong> This demonstrates the cognitive agent's ability to generate context-appropriate language for different communication partners. The UGV-U uses more natural language when reporting to the human operator.</p>
            </div>
          </div>
          
          <!-- Step 9 -->
          <div class="demo-step" id="step2-9">
            <div class="step-title">
              <span class="step-number">9</span>
              Mission Debrief: Team Coordination Summary
            </div>
            <div class="step-content">
              <p><strong>Mission Summary:</strong> The multi-robot search and retrieve mission demonstrates HARMONIC's distributed planning capabilities:</p>
              <ul>
                <li><strong>Collaborative Planning:</strong> Leader-subordinate coordination through meta-scripts</li>
                <li><strong>Distributed Execution:</strong> Strategic plans divided into tactical commands</li>
                <li><strong>Real-time Coordination:</strong> Continuous communication and status updates</li>
                <li><strong>Context-aware Communication:</strong> Different language styles for different audiences</li>
                <li><strong>Efficient Search:</strong> Coordinated coverage without redundancy</li>
              </ul>
              <p><strong>Key Success Factors:</strong> The mission succeeded due to effective plan distribution, clear role definitions, and robust communication protocols between the strategic and tactical layers.</p>
            </div>
          </div>
          
          <!-- Continue with more steps as needed... -->
          <!-- For now, I'll add a few more key steps to reach 30 total -->
          
          <!-- Steps 11-30 would continue the detailed breakdown -->
          <!-- I'll add a few more representative steps -->
          
          <!-- Step 10 - Demo Video -->
          <div class="demo-step" id="step2-10">
            <div class="step-title">
              <span class="step-number">10</span>
              Watch Full Demo - Multi-Robot
            </div>
            <div class="step-content">
              <p><strong>Complete HARMONIC Multi-Robot Demonstration</strong></p>
              <p>Watch the full multi-robot demonstration showing HARMONIC's distributed planning and collaborative execution process with UGV and drone coordination.</p>
              
              <!-- Full-width multi-robot video -->
              <div class="full-width-video-container" style="margin: 30px 0; width: 100%;">
                <video class="clickable-video full-demo-video" autoplay muted loop playsinline style="width: 100%; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);" onclick="openVideoModal('assets/videos/demos/demo3_multirobot_sim.mp4', 'Multi-Robot Search and Retrieve Demo')">
                  <source src="assets/videos/demos/demo3_multirobot_sim.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <div class="figure-caption">
                  <p><strong>Complete HARMONIC Multi-Robot Demo</strong></p>
                </div>
              </div>
              
            </div>
          </div>
          
        </div>
      </div>
      </section>
      
      <hr class="section-divider">

      <!-- 3 Minute Presentation Section -->
      <section id="presentation" style="margin: 40px 0; text-align: center;">
        <h2 style="color: var(--primary-color); font-size: 28px; font-weight: 600; margin-bottom: 20px;">3 Minute Presentation</h2>
        <div class="video-container" style="max-width: 1000px; margin: 0 auto;">
          <video controls muted autoplay loop style="width: 100%; border-radius: 8px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);">
            <source src="assets/videos/demos/ICRA26_4530_VI_i.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div class="figure-caption" style="margin-top: 15px;">
            <p><strong>Unmute for audio explanation</strong></p>
          </div>
        </div>
      </section>
      
      <hr class="section-divider">
      
      <!-- Books Section - REMOVED FOR BLINDED SUBMISSION -->
      <div style="margin-top: 30px; text-align: center;">
        <div class="placeholder" style="background-color: #fff3cd; border-color: #ffeaa7; color: #856404;">
          <strong>Related publications section removed for double-blind submission</strong>
        </div>
      </div>
      
      <hr class="section-divider">
      
      <!-- References and Glossary Container -->
      <div style="display: flex; gap: 30px; align-items: flex-start; flex-wrap: wrap;">
        
        <!-- References Section -->
        <section id="references" style="flex: 1; min-width: 300px;">
      <h2>References <button id="toggle-references" class="citation-button" style="margin-left: 15px; font-size: 14px;">
        <i class="fas fa-chevron-up"></i> Hide References
      </button></h2>
      
      <div id="references-container" style="display: block; max-width: 100%; max-height:560px; margin-top: 15px; background: var(--background-color); border-radius: 8px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.08); overflow-y: auto;">
        <ol style="padding-left: 20px; text-align: left; line-height: 1.4; font-size: 13px; color: var(--text-color); margin: 0; list-style-position: outside;">
            <li>Y. Tong, H. Liu, and Z. Zhang, "Advancements in humanoid robots: A comprehensive review and future prospects," <em>IEEE/CAA Journal of Automatica Sinica</em>, vol. 11, no. 2, pp. 301–328, 2024.</li>
            
            <li>B. Porter, "The problems with humanoid robots," 5 2024. [Online]. Available: https://medium.com/@bp64302/the-problems-with-humanoid-robots-9d8684d62008</li>
            
            <li>Y. Yang, L. Zhao, M. Ding, G. Bertasius, and D. Szafir, "BOSS: Benchmark for observation space shift in long-horizon task," <em>IEEE Robotics and Automation Letters</em>, vol. 10, no. 9, pp. 8882–8889, 2025.</li>
            
            <li>K. Kawaharazuka, T. Matsushima, A. Gambardella, J. Guo, C. Paxton, and A. Zeng, "Real-world robot applications of foundation models: a review," <em>Advanced Robotics</em>, vol. 38, no. 18, pp. 1232–1254, 2024.</li>
            
            <li>J. Bjorck, F. Castañeda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang et al., "Gr00t n1: An open foundation model for generalist humanoid robots," <em>arXiv preprint arXiv:2503.14734</em>, 2025.</li>
            
            <li>E. K. Raptis, A. C. Kapoutsis, and E. B. Kosmatopoulos, "Agentic LLM-based robotic systems for real-world applications: a review on their agenticness and ethics," <em>Frontiers in Robotics and AI</em>, vol. 12, p. 1605405, 2025.</li>
            
            <li>K. Zhang, R. Xu, P. Ren, J. Lin, H. Wu, L. Lin, and X. Liang, "Ro-Bridge: A hierarchical architecture bridging cognition and execution for general robotic manipulation," <em>arXiv</em>, 2025.</li>
            
            <li>M. Ahn, D. Dwibedi, C. Finn, M. G. Arenas, K. Gopalakrishnan, K. Hausman, and e. al., "AutoRT: Embodied foundation models for large scale orchestration of robotic agents," <em>arXiv</em>, 2024.</li>
            
            <li>M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, and e. al., "Do as i can, not as i say: Grounding language in robotic affordances," <em>arXiv</em>, 2022.</li>
            
            <li>B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, and e. al., "RT-2: Vision-language-action models transfer web knowledge to robotic control," in <em>Proceedings of The 7th Conference on Robot Learning</em>, ser. Proceedings of Machine Learning Research. PMLR, 2023.</li>
            
            <li>K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, and e. al., "π0: A vision-language-action flow model for general robot control." <em>arXiv preprint ARXIV.2410.24164</em>.</li>
            
            <li>M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, and e. al., "OpenVLA: An open-source vision-language-action model," <em>arXiv</em>, 2024.</li>
            
            <li>A. O'Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, and e. al., "Open x-embodiment: Robotic learning datasets and RT-x models," <em>2024 IEEE International Conference on Robotics and Automation (ICRA)</em>, vol. 00, pp. 6892–6903, 2024.</li>
            
            <li>Z. Bai, P. Wang, T. Xiao, T. He, Z. Han, Z. Zhang, and M. Z. Shou, "Hallucination of multimodal large language models: A survey," <em>arXiv</em>, 2024.</li>
            
            <li>F. Sun, R. Chen, T. Ji, Y. Luo, H. Zhou, and H. Liu, "A comprehensive survey on embodied intelligence: Advancements, challenges, and future perspectives," <em>CAAI Artificial Intelligence Research</em>, p. 9150042, 2024.</li>
            
            <li>A. Robey, Z. Ravichandran, V. Kumar, H. Hassani, and G. J. Pappas, "Jailbreaking LLM-controlled robots," <em>arXiv</em>, 2024.</li>
            
            <li>M. McShane, S. Nirenburg, and J. English, <em>Agents in the Long Game of AI: Computational Cognitive Modeling for Trustworthy, Hybrid AI</em>. Cambridge, MA: MIT Press, 2024.</li>
            
            <li>S. Nirenburg, M. McShane, K. Goodman, and S. Oruganti, "Explaining explaining," <em>Proceedings of the 1st International Conference on Explainable AI for Neural and Symbolic Methods</em>, pp. 116–123, 2024.</li>
            
            <li>S. Nirenburg, M. McShane, S. Beale, P. Wood, B. Scassellati, O. Magnin, and A. Roncone, "Toward human-like robot learning," in <em>International Conference on Applications of Natural Language to Information Systems</em>. Springer, 2018.</li>
            
            <li>L. A. Dennis, M. Fisher, N. K. Lincoln, A. Lisitsa, and S. M. Veres, "Practical verification of decision-making in agent-based autonomous systems," <em>Automated Software Engineering</em>, vol. 23, no. 3, pp. 305–359, 2016.</li>
            
            <li>M. Scheutz, G. Briggs, R. Cantrell, E. Krause, T. Williams, and R. Veale, "Novel mechanisms for natural human-robot interactions in the diarc architecture," in <em>Proceedings of AAAI workshop on intelligent robotic systems</em>. Palo Alto, CA, 2013, p. 66.</li>
            
            <li>P. W. Schermerhorn, J. F. Kramer, C. Middendorff, and M. Scheutz, "Diarc: A testbed for natural human-robot interaction." in <em>AAAI</em>, vol. 6, 2006, pp. 1972–1973.</li>
            
            <li>J. English and S. Nirenburg, "OntoAgent: Implementing content-centric cognitive models," in <em>Proceedings of the Annual Conference on Advances in Cognitive Systems</em>, ser. Eighth Annual Conference on Advances in Cognitive Systems, 2020.</li>
            
            <li>M. McShane and S. Nirenburg, <em>Linguistics for the Age of AI</em>. Cambridge, MA: MIT Press, 2021.</li>
            
            <li>S. Nirenburg, T. Ferguson, and M. McShane, "Mutual trust in human-ai teams relies on metacognition," in <em>Metacognitive Artificial Intelligence</em>, Wei, \. Shakarian, Hui and\ , and Paulo, Eds. Cambridge University Press, 2024.</li>
            
            <li>M. Colledanchise and P. Ögren, <em>Behavior Trees in Robotics and AI: An Introduction</em>. CRC Press, 2018.</li>
            
            <li>M. Iannotta, D. C. Domínguez, J. A. Stork, E. Schaffernicht, and T. Stoyanov, "Heterogeneous full-body control of a mobile manipulator with behavior trees," <em>arXiv preprint arXiv:2210.08600</em>, 2022.</li>
            
            <li>S. S. O. Venkata, R. Parasuraman, and R. Pidaparti, "Kt-bt: A framework for knowledge transfer through behavior trees in multirobot systems," <em>IEEE Transactions on Robotics</em>, vol. 39, no. 5, pp. 4114–4130, 2023.</li>
            
            <li>M. Colledanchise and L. Natale, "On the implementation of behavior trees in robotics," <em>IEEE Robotics and Automation Letters</em>, vol. 6, no. 3, pp. 5929–5936, 2021.</li>
            
            <li>D. Kahneman, <em>Thinking, Fast and Slow</em>. Macmillan, 2011.</li>
            
            <li>J. E. Laird, <em>The Soar Cognitive Architecture</em>. Cambridge, MA: MIT Press, 2012. [Online]. Available: https://mitpress.mit.edu/9780262122962/the-soar-cognitive-architecture/</li>
            
            <li>F. E. Ritter, J. R. Anderson et al., "Act-r: A cognitive architecture for modeling cognition," <em>Wiley Interdisciplinary Reviews: Cognitive Science</em>, vol. 10, no. 3, p. e1488, 2019. [Online]. Available: https://doi.org/10.1002/wcs.1488</li>
            
            <li>M. Scheutz, T. Williams, E. Krause, B. Oosterveld, V. Sarathy, and T. Frasca, "An overview of the distributed integrated cognition affect and reflection diarc architecture," <em>Intelligent Systems, Control and Automation: Science and Engineering</em>, pp. 165–193, 2019.</li>
            
            <li>J. E. Laird, P. S. Rosenbloom, and A. Newell, "Robo-soar: An integration of external interaction, planning, and learning using soar," <em>Robotics and Autonomous Systems</em>, vol. 8, no. 1-2, pp. 113–129, 1991. [Online]. Available: https://doi.org/10.1016/0921-8890(91)90017-F</li>
            
            <li>J.-Y. Puigbo, A. Pumarola, C. Angulo, and R. Tellez, "Using a cognitive architecture for general purpose service robot control," <em>Connection Science</em>, vol. 27, no. 2, pp. 105–117, 2015. [Online]. Available: https://doi.org/10.1080/09540091.2014.968093</li>
            
            <li>S. Harnad, "The symbol grounding problem," <em>Physica D: Nonlinear Phenomena</em>, vol. 42, no. 1-3, pp. 335–346, 1990. [Online]. Available: https://doi.org/10.1016/0167-2789(90)90087-6</li>
            
            <li>J. G. Trafton, L. M. Hiatt, A. M. Harrison, I. Franklin P. Tamborello, S. S. Khemlani, and A. C. Schultz, "Act-r/e: An embodied cognitive architecture for human–robot interaction," <em>Journal of Human-Robot Interaction</em>, vol. 2, no. 1, pp. 30–55, 2013. [Online]. Available: https://doi.org/10.5898/JHRI.2.1.Trafton</li>
            
            <li>M. Scheutz, R. Cantrell, and P. Schermerhorn, "Toward humanlike task-based dialogue processing for human robot interaction," <em>AI Magazine</em>, vol. 32, no. 4, pp. 77–84, 2011.</li>
        </ol>
        <div style="text-align: right; margin-top: 15px;">
          <button class="copy-button" onclick="copyReferences()" style="background: var(--accent-color); color: white; border: none; border-radius: 6px; padding: 8px 16px; font-size: 13px; cursor: pointer; transition: all 0.2s ease;">Copy All</button>
        </div>
      </div>
      </section>
      
        <!-- Glossary Section -->
        <section id="glossary" style="flex: 1; min-width: 310px;">
      <h2>Glossary <button id="toggle-glossary" class="citation-button" style="margin-left: 15px; font-size: 14px;">
        <i class="fas fa-chevron-up"></i> Hide Glossary
      </button></h2>
      
      <!-- Search Box - Outside scrollable area -->
      <div style="margin: 15px 0; text-align: center;">
        <input type="text" id="glossary-search" placeholder="🔍 Search glossary terms..." 
               style="width: 70%; max-width: 400px; padding: 12px 18px; border: 2px solid var(--accent-color); border-radius: 20px; font-size: 15px; background: white; color: var(--text-color); box-shadow: 0 2px 8px rgba(0,0,0,0.1); transition: all 0.3s ease;">
      </div>
      
      <!-- Alphabet Directory - Outside scrollable area -->
      <div style="margin-bottom: 15px; text-align: center;">
        <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 8px; max-width: 600px; margin: 0 auto;">
          <button class="alphabet-btn" data-letter="all" style="background: var(--accent-color); color: white; border: none; border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">All</button>
          <button class="alphabet-btn" data-letter="a" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">A</button>
          <button class="alphabet-btn" data-letter="b" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">B</button>
          <button class="alphabet-btn" data-letter="c" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">C</button>
          <button class="alphabet-btn" data-letter="d" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">D</button>
          <button class="alphabet-btn" data-letter="e" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">E</button>
          <button class="alphabet-btn" data-letter="g" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">G</button>
          <button class="alphabet-btn" data-letter="h" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">H</button>
          <button class="alphabet-btn" data-letter="l" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">L</button>
          <button class="alphabet-btn" data-letter="m" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">M</button>
          <button class="alphabet-btn" data-letter="o" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">O</button>
          <button class="alphabet-btn" data-letter="p" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">P</button>
          <button class="alphabet-btn" data-letter="r" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">R</button>
          <button class="alphabet-btn" data-letter="s" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">S</button>
          <button class="alphabet-btn" data-letter="t" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">T</button>
          <button class="alphabet-btn" data-letter="v" style="background: white; color: var(--text-color); border: 2px solid var(--border-color); border-radius: 6px; padding: 8px 12px; font-size: 14px; font-weight: 600; cursor: pointer; transition: all 0.2s ease; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">V</button>
        </div>
      </div>
      
      <div id="glossary-container" style="display: block; max-width: 100%; max-height: 405px; background: var(--background-color); border-radius: 12px; padding: 25px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); overflow-y: auto;">
        
        <!-- Glossary Content -->
        <div id="glossary-content" style="max-width: 900px; margin: 0 auto;">
          
          <!-- A Section -->
          <div class="alphabet-section" data-letter="a">
            <div class="alphabet-header">A</div>
            
            <div class="glossary-term" data-term="action meaning representation amr">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">AMR (Action Meaning Representation)</h4>
                    <p class="glossary-definition">Semantic representations specifying the content of actions to be executed, generated by the decision-making service before being rendered into executable commands.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="action data communication apis">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Action Data Communication APIs</h4>
                    <p class="glossary-definition">The strategic and tactical layers are connected through a bidirectional interface that enables efficient inter-layer communication and data transfer. High-level action commands sent by the OntoAgent through the Interface APIs are unpacked and used to update corresponding variables in the blackboard.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="action outputs">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Action Outputs</h4>
                    <p class="glossary-definition">Physical and communicative outputs generated by the system. Represents the system's ability to interact with and affect the environment.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="actuation and motor control policies">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Actuation and Motor Control Policies</h4>
                    <p class="glossary-definition">Specific rules and algorithms for controlling physical outputs. Implements safety constraints and control policies for physical actions.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="attention service">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Attention Service</h4>
                    <p class="glossary-definition">Focuses cognitive resources on relevant information for strategic decision-making and directs sensory focus for immediate task relevance. Manages information filtering and prioritization at both cognitive and tactical levels.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="actionability assessment">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Actionability Assessment</h4>
                    <p class="glossary-definition">The process of determining whether an agent has sufficient understanding of a situation to proceed with action despite potential incomplete information.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="adjacency pairs">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">A</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Adjacency Pairs</h4>
                    <p class="glossary-definition">Discourse-level action-response patterns governing turn-taking behaviors in dialog (e.g., request-compliance, question-answer, proposal-acceptance sequences).</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- B Section -->
          <div class="alphabet-section" data-letter="b">
            <div class="alphabet-header">B</div>
            
            <div class="glossary-term" data-term="behavior trees bts" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">B</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Behavior Trees (BTs)</h4>
                    <p class="glossary-definition">Hierarchical structures in the tactical layer that execute operations with priority ordering, enabling reactive responses while maintaining planned behavior.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="bidirectional interface" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">B</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Bidirectional Interface</h4>
                    <p class="glossary-definition">The communication channel between strategic and tactical layers enabling data transfer and command execution across the dual-control architecture.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- C Section -->
          <div class="alphabet-section" data-letter="c">
            <div class="alphabet-header">C</div>
            
            <div class="glossary-term" data-term="collaborative activity script" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">C</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Collaborative-Activity Script</h4>
                    <p class="glossary-definition">A meta-script that helps agents operating in teams organize themselves to accomplish shared goals, with different versions for team leaders and subordinates.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="common ground" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">C</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Common Ground</h4>
                    <p class="glossary-definition">Shared understanding between team members about goals, plans, and situational awareness, essential for effective collaboration.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="communicative acts" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">C</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Communicative Acts</h4>
                    <p class="glossary-definition">The intended function or purpose of an utterance (broader than speech acts as it includes non-linguistic communication), such as requests, assertions, or questions.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- D Section -->
          <div class="alphabet-section" data-letter="d">
            <div class="alphabet-header">D</div>
            
            <div class="glossary-term" data-term="discourse relations" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">D</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Discourse Relations</h4>
                    <p class="glossary-definition">Semantic relationships between propositions in dialog turns that may be explicit or inferred, important for assessing actionability.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- E Section -->
          <div class="alphabet-section" data-letter="e">
            <div class="alphabet-header">E</div>
            
            <div class="glossary-term" data-term="episodic memory" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">E</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Episodic Memory</h4>
                    <p class="glossary-definition">Long-term storage of remembered instances of world objects, events, and past processing experiences, enabling agents to leverage past experiences for future decision-making.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="explainability" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">E</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Explainability</h4>
                    <p class="glossary-definition">The system's ability to provide transparent, human-understandable explanations of its reasoning, decisions, and actions through traces of cognitive processing.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- G Section -->
          <div class="alphabet-section" data-letter="g">
            <div class="alphabet-header">G</div>
            
            <div class="glossary-term" data-term="gmr generation meaning representation" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">G</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">GMR (Generation Meaning Representation)</h4>
                    <p class="glossary-definition">Intermediate semantic specifications for language generation, encoding communicative content grounded in ontological concepts prior to surface realization through the natural language generator.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="grounding" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">G</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Grounding</h4>
                    <p class="glossary-definition">The process of connecting symbolic representations to physical world entities, perceptual data, or episodic memory instances (indicated by # indices in representations).</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- H Section -->
          <div class="alphabet-section" data-letter="h">
            <div class="alphabet-header">H</div>
            
            <div class="glossary-term" data-term="harmonic human ai robotic team member" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">H</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">HARMONIC (Human-AI Robotic Team Member Operating with Natural Intelligence and Communication)</h4>
                    <p class="glossary-definition">A dual-control cognitive-robotic architecture that integrates strategic (cognitive) level decision-making with tactical (robotic) level control, enabling robots to function as trusted teammates in human-robot teams through transparent reasoning and natural language communication.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- L Section -->
          <div class="alphabet-section" data-letter="l">
            <div class="alphabet-header">L</div>
            
            <div class="glossary-term" data-term="leia language endowed intelligent agent" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">L</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">LEIA (Language-Endowed Intelligent Agent)</h4>
                    <p class="glossary-definition">The cognitive architecture incorporated in the strategic layer (aka OntoAgent). Neurosymbolic, multimodal cognitive-robotic systems implemented in the HARMONIC architecture that can interpret experiences, reason, and learn using ontologically-grounded knowledge. This is used interchageably with OntoAgent.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- M Section -->
          <div class="alphabet-section" data-letter="m">
            <div class="alphabet-header">M</div>
            
            <div class="glossary-term" data-term="metacognitive reasoning" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">M</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Metacognitive Reasoning</h4>
                    <p class="glossary-definition">Self-monitoring capabilities enabling introspection of internal states, team member modeling (mindreading), and dynamic strategy adjustment based on situational assessment.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="mindreading" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">M</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Mindreading</h4>
                    <p class="glossary-definition">The metacognitive capability of modeling teammates' mental states, beliefs, capabilities, and intentions to enable effective collaboration.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- O Section -->
          <div class="alphabet-section" data-letter="o">
            <div class="alphabet-header">O</div>
            
            <div class="glossary-term" data-term="ontoagent" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">O</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">OntoAgent</h4>
                    <p class="glossary-definition">The cognitive architecture incorporated in the strategic layer (aka LEIA), responsible for semantic interpretation, attention management, goal-setting, sophisticated planning, and addressing unexpected challenges in interpretable ways. This is used interchageably with LEIA.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="ontograph" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">O</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">OntoGraph</h4>
                    <p class="glossary-definition">A knowledge base API providing unified format for representing and accessing knowledge across the system, supporting inheritance, flexible organization into "spaces," and efficient querying.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="ontology" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">O</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Ontology</h4>
                    <p class="glossary-definition">A hierarchical knowledge repository containing formalized representations of entities (concepts), relationships, properties, and procedural schemas (scripts) that serve as the semantic foundation for agent reasoning.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- P Section -->
          <div class="alphabet-section" data-letter="p">
            <div class="alphabet-header">P</div>
            
            <div class="glossary-term" data-term="perception interpretation" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Perception Interpretation</h4>
                    <p class="glossary-definition">The process of converting multimodal sensory inputs (speech, vision, haptic) into ontologically-grounded meaning representations for unified reasoning.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="perception data communication apis">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Perception Data Communication APIs</h4>
                    <p class="glossary-definition">The tactical layer provides the strategic layer with preprocessed multimodal (speech, vision, etc.) perception data and relays robot state information, employing a suite of perception models within the perception services.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="perception inputs">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Perception Inputs</h4>
                    <p class="glossary-definition">Various sensory data streams feeding into the system. Provides the raw information needed for environmental understanding and decision-making.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="perception services tactical">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Perception Services (Tactical)</h4>
                    <p class="glossary-definition">Handles raw sensory input and provides real-time environmental awareness. Processes sensor data for immediate action and safety monitoring. Across the two systems, we use three different robots: a UGV and a Drone in simulation environments, and a tabletop serial manipulator. Each robot operates its own instance of OntoAgent at the strategic level and customized low-level planners at the tactical level.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="perception and interpretation">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Perception and Interpretation</h4>
                    <p class="glossary-definition">Low-level processing for environmental sensing and map creation. Handles SLAM, object detection, and spatial understanding.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="plans" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Plans</h4>
                    <p class="glossary-definition">Instances of scripts with parameter values set for specific situations, executed through the goal and plan agenda.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="preconditions" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">P</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Preconditions</h4>
                    <p class="glossary-definition">Requirements that must be satisfied before a plan can be executed, checked and resolved during plan instantiation.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- R Section -->
          <div class="alphabet-section" data-letter="r">
            <div class="alphabet-header">R</div>
            
            <div class="glossary-term" data-term="reference resolution" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">R</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Reference Resolution</h4>
                    <p class="glossary-definition">True resolution of referring expressions to specific instances in episodic memory, distinguished from textual coreference resolution.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- S Section -->
          <div class="alphabet-section" data-letter="s">
            <div class="alphabet-header">S</div>
            
            <div class="glossary-term" data-term="scripts" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">S</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Scripts</h4>
                    <p class="glossary-definition">Complex events or procedural knowledge recorded as sequences of events with coreferenced participants and props, representing how typical actions unfold.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="short horizon plans">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">S</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Short Horizon Plans</h4>
                    <p class="glossary-definition">Tactical plans for immediate future actions, often derived from strategic goals. Bridges the gap between high-level planning and immediate execution.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="strategic reasoning decision making">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">S</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Strategic Reasoning & Decision-Making</h4>
                    <p class="glossary-definition">Generates high-level plans and makes decisions based on long-term goals and knowledge. Handles complex reasoning about goals, constraints, and trade-offs.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="semantic memory" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">S</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Semantic Memory</h4>
                    <p class="glossary-definition">Long-term storage of generic knowledge about types of events and objects, distinct from specific instances stored in episodic memory.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="situation model" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">S</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Situation Model</h4>
                    <p class="glossary-definition">Working memory containing currently active concept instances and representations of entities and events that are part of the current task context.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="strategic layer" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">S</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Strategic Layer</h4>
                    <p class="glossary-definition">The cognitive component responsible for high-level decision-making, planning, perception interpretation, attention management, and goal selection. Implements System 2 (slow, deliberative reasoning) processing.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- T Section -->
          <div class="alphabet-section" data-letter="t">
            <div class="alphabet-header">T</div>
            
            <div class="glossary-term" data-term="tactical layer" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">T</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Tactical Layer</h4>
                    <p class="glossary-definition">The robotic control component responsible for low-level robot control, execution of motor actions, reflexive attention, sensor processing, and reactive behaviors. Implements System 1 (fast, reflexive) processing.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="tactical reasoning decision making">
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">T</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Tactical Reasoning & Decision-Making</h4>
                    <p class="glossary-definition">In the action translation pipeline of the tactical layer, specialized robotic planners, controllers, algorithms, and action sequence models are employed as part of the Reasoning and Decision Making modules and Effector Services to translate abstract action commands from OntoAgent into precise, executable robot control operations.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="team leader" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">T</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">Team Leader</h4>
                    <p class="glossary-definition">The robot designated to manage interactions with human operators, select domain scripts, resolve preconditions, and coordinate with subordinate robots.</p>
                  </div>
                </div>
              </div>
              
              <div class="glossary-term" data-term="tmr text meaning representation" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">T</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">TMR (Text Meaning Representation)</h4>
                    <p class="glossary-definition">Ontologically-grounded semantic structures encoding interpreted linguistic input, preserving speaker intent, propositional content, and pragmatic markers.</p>
                  </div>
                </div>
              </div>
              
          </div>
          
          <!-- V Section -->
          <div class="alphabet-section" data-letter="v">
            <div class="alphabet-header">V</div>
            
            <div class="glossary-term" data-term="vmr visual meaning representation" >
                <div style="display: flex; align-items: flex-start; gap: 10px;">
                  <div class="glossary-badge">V</div>
                  <div class="glossary-content">
                    <h4 class="glossary-title">VMR (Visual Meaning Representation)</h4>
                    <p class="glossary-definition">Ontologically-grounded interpretations of visual perceptual data, unifying multimodal sensory inputs into symbolic knowledge structures similar in form to TMRs.</p>
                  </div>
                </div>
              </div>
          </div>
            
        </div>
        </section>
        
      </div>
      
      <hr class="section-divider">
      
      <!-- Acknowledgments Section - BLINDED FOR SUBMISSION -->
      <h2>Acknowledgments</h2>
      <div class="placeholder" style="background-color: #fff3cd; border-color: #ffeaa7; color: #856404;">
        <strong>Acknowledgments removed for double-blind submission</strong>
        <br><br>
        Funding information and acknowledgments will be added after acceptance.
      </div>
      
    </div>
    
    <!-- Footer - BLINDED FOR SUBMISSION -->
    <div class="footer">
      © 2026 HARMONIC Project Team. All rights reserved.
    </div>
  </div>
  
  <script>
    // Toggle citation display
    function toggleCitation(id) {
      const element = document.getElementById(id);
      const allCitations = document.querySelectorAll('.citation-content');
      
      // Hide all other citations first
      allCitations.forEach(citation => {
        if (citation.id !== id) {
          citation.style.display = 'none';
        }
      });
      
      // Toggle the selected citation
      if (element.style.display === 'block') {
        element.style.display = 'none';
      } else {
        element.style.display = 'block';
      }
    }
    
    // Copy text function for BibTeX
    function copyText(id) {
      const element = document.getElementById(id);
      const text = element.innerText.replace('Copy', '').trim();
      
      navigator.clipboard.writeText(text)
        .then(() => {
          const button = element.querySelector('.copy-button');
          const originalText = button.innerText;
          
          button.innerText = 'Copied!';
          setTimeout(() => {
            button.innerText = originalText;
          }, 2000);
        })
        .catch(err => {
          console.error('Could not copy text: ', err);
        });
    }
    
    // Copy individual citation format
    function copyCitationFormat(id) {
      const element = document.getElementById(id);
      const text = element.innerText.trim();
      
      navigator.clipboard.writeText(text)
        .then(() => {
          const parent = element.closest('div');
          const button = parent.querySelector('.copy-button');
          const originalText = button.innerText;
          
          button.innerText = 'Copied!';
          setTimeout(() => {
            button.innerText = originalText;
          }, 2000);
        })
        .catch(err => {
          console.error('Could not copy citation format: ', err);
        });
    }
    
    // Copy references function
    function copyReferences() {
      const referencesContainer = document.getElementById('references-container');
      const referencesList = referencesContainer.querySelector('ol').innerText;
      
      navigator.clipboard.writeText(referencesList)
        .then(() => {
          const button = referencesContainer.querySelector('.copy-button');
          const originalText = button.innerText;
          
          button.innerText = 'Copied!';
          setTimeout(() => {
            button.innerText = originalText;
          }, 2000);
        })
        .catch(err => {
          console.error('Could not copy references: ', err);
        });
    }
    
    // Generate citation formats from BibTeX
    function generateCitationFormats() {
      // Get BibTeX content
      const bibtexElement = document.getElementById('bibtex-citation');
      const bibtexText = bibtexElement.innerText.replace('Copy', '').trim();
      
      // Parse BibTeX
      const titleMatch = bibtexText.match(/title=\{([^}]+)\}/);
      const authorMatch = bibtexText.match(/author=\{([^}]+)\}/);
      const yearMatch = bibtexText.match(/year=\{(\d+)\}/);
      const urlMatch = bibtexText.match(/url=\{([^}]+)\}/);
      const eprintMatch = bibtexText.match(/eprint=\{([^}]+)\}/);
      
      if (titleMatch && authorMatch && yearMatch) {
        const title = titleMatch[1];
        const authors = authorMatch[1].split(' and ');
        const year = yearMatch[1];
        const url = urlMatch ? urlMatch[1] : '';
        const eprint = eprintMatch ? eprintMatch[1] : '';
        
        // Format authors for different citation styles
        let authorLastFirst = [];
        
        authors.forEach(author => {
          // Attempt to split "Lastname, Firstname" or "Lastname Firstname"
          const parts = author.trim().split(/,\s*|\s+/);
          if (parts.length >= 2) {
            const lastName = parts[0];
            const initials = parts.slice(1).join(' ');
            authorLastFirst.push(`${lastName}, ${initials}`);
          } else {
            authorLastFirst.push(author);
          }
        });
        
        // APA format
        let apaAuthors = '';
        if (authors.length === 1) {
          apaAuthors = authorLastFirst[0];
        } else if (authors.length === 2) {
          apaAuthors = `${authorLastFirst[0]} & ${authorLastFirst[1]}`;
        } else if (authors.length > 2) {
          apaAuthors = authorLastFirst.slice(0, authors.length - 1).join(', ') + ', & ' + authorLastFirst[authors.length - 1];
        }
        
        // Replace full first names with initials in APA format
        apaAuthors = apaAuthors.replace(/(\w+),\s+(\w+)/g, (match, lastName, firstName) => {
          return `${lastName}, ${firstName.charAt(0)}.`;
        });
        
        const apaText = `${apaAuthors} (${year}). ${title}. <i>arXiv preprint arXiv:${eprint}</i>.${url ? ` ${url}` : ''}`;
        document.getElementById('apa-text').innerHTML = apaText;
        
        // MLA format
        let mlaAuthors = '';
        if (authors.length === 1) {
          mlaAuthors = authorLastFirst[0];
        } else if (authors.length > 1) {
          mlaAuthors = `${authorLastFirst[0]}, et al.`;
        }
        
        const mlaText = `${mlaAuthors}. "${title}." <i>arXiv preprint arXiv:${eprint}</i> (${year}).`;
        document.getElementById('mla-text').innerHTML = mlaText;
        
        // Chicago format
        let chicagoAuthors = '';
        if (authors.length === 1) {
          chicagoAuthors = authorLastFirst[0];
        } else if (authors.length === 2) {
          chicagoAuthors = `${authorLastFirst[0]} and ${authorLastFirst[1]}`;
        } else if (authors.length > 2) {
          chicagoAuthors = authorLastFirst.slice(0, authors.length - 1).join(', ') + ', and ' + authorLastFirst[authors.length - 1];
        }
        
        const chicagoText = `${chicagoAuthors}. "${title}." <i>arXiv preprint arXiv:${eprint}</i> (${year}).`;
        document.getElementById('chicago-text').innerHTML = chicagoText;
      }
    }
    
    // Toggle references section
    document.addEventListener('DOMContentLoaded', function() {
      const toggleButton = document.getElementById('toggle-references');
      const referencesContainer = document.getElementById('references-container');
      const glossaryContainer = document.getElementById('glossary-container');
      const toggleGlossaryButton = document.getElementById('toggle-glossary');
      
      toggleButton.addEventListener('click', function() {
        if (referencesContainer.style.display === 'block') {
          referencesContainer.style.display = 'none';
          toggleButton.innerHTML = '<i class="fas fa-chevron-down"></i> Show References';
        } else {
          referencesContainer.style.display = 'block';
          toggleButton.innerHTML = '<i class="fas fa-chevron-up"></i> Hide References';
        }
      });
      
      // Toggle glossary visibility
      toggleGlossaryButton.addEventListener('click', function() {
        if (glossaryContainer.style.display === 'block') {
          glossaryContainer.style.display = 'none';
          toggleGlossaryButton.innerHTML = '<i class="fas fa-chevron-down"></i> Show Glossary';
        } else {
          glossaryContainer.style.display = 'block';
          toggleGlossaryButton.innerHTML = '<i class="fas fa-chevron-up"></i> Hide Glossary';
        }
      });
      
      // Glossary search functionality
      const glossarySearch = document.getElementById('glossary-search');
      const glossaryTerms = document.querySelectorAll('.glossary-term');
      
      if (glossarySearch) {
        glossarySearch.addEventListener('input', function() {
          const searchTerm = this.value.toLowerCase().trim();
          const alphabetSections = document.querySelectorAll('.alphabet-section');
          
          // If search is active, show all sections and filter terms
          if (searchTerm.length > 0) {
            alphabetSections.forEach(function(section) {
              section.style.display = 'block';
            });
            
            glossaryTerms.forEach(function(term) {
              const termData = term.getAttribute('data-term').toLowerCase();
              const termText = term.textContent.toLowerCase();
              
              if (termData.includes(searchTerm) || termText.includes(searchTerm)) {
                term.style.display = 'block';
                term.style.marginBottom = '8px';
              } else {
                term.style.display = 'none';
                term.style.marginBottom = '0px';
              }
            });
          } else {
            // If search is empty, restore alphabet filtering
            filterByAlphabet(currentAlphabetFilter);
          }
        });
      }
      
      // Alphabet directory functionality
      let currentAlphabetFilter = 'all';
      const alphabetButtons = document.querySelectorAll('.alphabet-btn');
      
      alphabetButtons.forEach(function(button) {
        button.addEventListener('click', function() {
          const letter = this.getAttribute('data-letter');
          
          // Update active button
          alphabetButtons.forEach(function(btn) {
            btn.classList.remove('active');
            btn.style.background = 'white';
            btn.style.color = 'var(--text-color)';
            btn.style.borderColor = 'var(--border-color)';
          });
          
          this.classList.add('active');
          this.style.background = 'var(--accent-color)';
          this.style.color = 'white';
          this.style.borderColor = 'var(--accent-color)';
          
          // Filter glossary
          filterByAlphabet(letter);
          currentAlphabetFilter = letter;
          
          // Clear search if filtering by alphabet
          if (glossarySearch) {
            glossarySearch.value = '';
          }
        });
      });
      
      // Function to filter glossary by alphabet
      function filterByAlphabet(letter) {
        const alphabetSections = document.querySelectorAll('.alphabet-section');
        const glossaryTerms = document.querySelectorAll('.glossary-term');
        
        if (letter === 'all') {
          // Show all sections and terms
          alphabetSections.forEach(function(section) {
            section.style.display = 'block';
          });
          glossaryTerms.forEach(function(term) {
            term.style.display = 'block';
            term.style.marginBottom = '8px';
          });
        } else {
          // Hide all sections first
          alphabetSections.forEach(function(section) {
            section.style.display = 'none';
          });
          
          // Show only the selected letter section
          const targetSection = document.querySelector(`.alphabet-section[data-letter="${letter}"]`);
          if (targetSection) {
            targetSection.style.display = 'block';
          }
          
          // Show all terms in the selected section
          glossaryTerms.forEach(function(term) {
            term.style.display = 'block';
            term.style.marginBottom = '8px';
          });
        }
      }
      
      // Hover effects are now handled by CSS
      
      // Auto-expand sections when navigation links are clicked
      const navLinks = document.querySelectorAll('.nav-link');
      navLinks.forEach(function(link) {
        link.addEventListener('click', function(e) {
          const href = this.getAttribute('href');
          
          // Handle glossary link
          if (href === '#glossary') {
            e.preventDefault();
            // Use existing variables
            
            // Scroll to glossary section
            document.getElementById('glossary').scrollIntoView({ 
              behavior: 'smooth',
              block: 'start'
            });
            
            // Auto-expand glossary if it's collapsed
            if (glossaryContainer.style.display === 'none') {
              glossaryContainer.style.display = 'block';
              toggleGlossaryButton.innerHTML = '<i class="fas fa-chevron-up"></i> Hide Glossary';
            }
          }
          
          // Handle references link
          if (href === '#references') {
            e.preventDefault();
            // Use existing variables
            
            // Scroll to references section
            document.getElementById('references').scrollIntoView({ 
              behavior: 'smooth',
              block: 'start'
            });
            
            // Auto-expand references if it's collapsed
            if (referencesContainer.style.display === 'none' || referencesContainer.style.display === '') {
              referencesContainer.style.display = 'block';
              toggleReferencesButton.innerHTML = '<i class="fas fa-chevron-up"></i> Hide References';
            }
          }
        });
      });
      
      // Generate citation formats on page load
      generateCitationFormats();
      
      // Add event listener to BibTeX to update citation formats when it changes
      const bibtexTextarea = document.getElementById('bibtex-citation');
      if (bibtexTextarea) {
        const observer = new MutationObserver(function() {
          generateCitationFormats();
        });
        
        observer.observe(bibtexTextarea, { 
          characterData: true,
          childList: true,
          subtree: true 
        });
      }
    });
    
    // Interactive Demo Functions
    let currentStepIndex = 0;
    const totalSteps = 31;
    let autoPlayInterval = null;
    
    // Step titles for navigation
    const stepTitles = [
      'Understanding HARMONIC\'s Reasoning Process',
      'Human Input: "It looks like the engine is overheating."',
      'Adjacency Pair Activation',
      'Plan Generation',
      'Action Procedure Definition',
      'Plan Execution',
      'Response Generation',
      'Follow-up Question',
      'Adjacency Pair Activation',
      'Plan Generation',
      'Action Procedure Definition',
      'Plan Execution',
      'Action Request',
      'Adjacency Pair Activation',
      'Plan Generation',
      'Multi-Step Plan Definition',
      'Precondition Check',
      'Information Request Plan',
      'Sub-Plan Creation',
      'Plan Activation',
      'Plan Activation (Continued)',
      'Human Response',
      'Plan Status Update',
      'Search Initiation',
      'Wait for Object Location',
      'Object Detection',
      'Object Pickup',
      'Navigation',
      'Delivery',
      'Watch Full Demo - Simulation',
      'Watch Full Demo - Real-Robot'
    ];
    
    function initializeInteractiveDemo() {
      generateStepNavigation();
      updateStepDisplay();
    }
    
    function generateStepNavigation() {
      const stepNavGrid = document.getElementById('stepNavGrid');
      stepNavGrid.innerHTML = '';
      
      for (let i = 0; i < totalSteps; i++) {
        const button = document.createElement('button');
        
        // Special styling for demo steps
        if (i === 29) {
          button.className = 'step-nav-button simulation-nav-button';
          button.textContent = '29';
          button.title = `Step ${i}: ${stepTitles[i]}`;
        } else if (i === 30) {
          button.className = 'step-nav-button robot-nav-button';
          button.textContent = '30';
          button.title = `Step ${i}: ${stepTitles[i]}`;
        } else {
          button.className = 'step-nav-button';
          button.textContent = i === 0 ? 'Intro' : i;
          button.title = `Step ${i}: ${stepTitles[i]}`;
        }
        
        button.onclick = () => goToStep(i);
        stepNavGrid.appendChild(button);
      }
    }
    
    function updateStepNavigation() {
      const navButtons = document.querySelectorAll('#stepNavGrid .step-nav-button');
      navButtons.forEach((button, index) => {
        const stepNumber = index;
        button.classList.remove('active', 'completed');
        
        if (stepNumber === currentStepIndex) {
          button.classList.add('active');
        } else if (stepNumber < currentStepIndex) {
          button.classList.add('completed');
        }
      });
    }
    
    function goToStep(stepNumber) {
      if (stepNumber >= 0 && stepNumber < totalSteps) {
        currentStepIndex = stepNumber;
        updateStepDisplay(false); // No scroll when user clicks step buttons
        updateStepNavigation();
        
        // Stop auto-play if it's running
        if (autoPlayInterval) {
          clearInterval(autoPlayInterval);
          autoPlayInterval = null;
        }
      }
    }
    
    function updateStepDisplay(shouldScroll = false) {
      // Update step counter
      document.getElementById('currentStep').textContent = currentStepIndex;
      document.getElementById('totalSteps').textContent = totalSteps - 1;
      
      // Update button states
      document.getElementById('prevStep').disabled = currentStepIndex <= 0;
      document.getElementById('nextStep').disabled = currentStepIndex >= totalSteps - 1;
      
      // Hide all steps first (only from demo 1)
      document.querySelectorAll('#demoContent .demo-step').forEach(step => {
        step.classList.remove('active', 'completed');
        step.style.display = 'none';
      });
      
      // Show only the current step
      const currentStep = document.getElementById(`step${currentStepIndex}`);
      if (currentStep) {
        currentStep.classList.add('active');
        currentStep.style.display = 'block';
      }
      
      // Update step navigation
      updateStepNavigation();
      
      // Only scroll when explicitly requested (user navigation)
      if (shouldScroll) {
        const demoPanel = document.querySelector('.interactive-demo');
        if (demoPanel) {
          demoPanel.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      }
    }
    
    function nextStep() {
      if (currentStepIndex < totalSteps - 1) {
        currentStepIndex++;
        updateStepDisplay(true); // Scroll when user navigates
      }
    }
    
    function previousStep() {
      if (currentStepIndex > 0) {
        currentStepIndex--;
        updateStepDisplay(true); // Scroll when user navigates
      }
    }
    
    function toggleAutoPlay() {
      const autoPlayBtn = document.getElementById('autoPlayBtn');
      
      if (autoPlayInterval) {
        // Stop auto-play
        clearInterval(autoPlayInterval);
        autoPlayInterval = null;
        autoPlayBtn.textContent = 'Auto Play';
        autoPlayBtn.style.backgroundColor = '';
      } else {
        // Start auto-play
        autoPlayInterval = setInterval(() => {
          if (currentStepIndex < totalSteps - 1) {
            nextStep();
          } else {
            // Reached the end, stop auto-play
            clearInterval(autoPlayInterval);
            autoPlayInterval = null;
            autoPlayBtn.textContent = 'Auto Play';
            autoPlayBtn.style.backgroundColor = '';
          }
        }, 3000); // 3 seconds per step
        
        autoPlayBtn.textContent = 'Stop';
        autoPlayBtn.style.backgroundColor = '#d32f2f';
      }
    }
    
    // Keyboard navigation
    document.addEventListener('keydown', function(e) {
      if (e.key === 'ArrowRight' || e.key === ' ') {
        e.preventDefault();
        nextStep();
      } else if (e.key === 'ArrowLeft') {
        e.preventDefault();
        previousStep();
      } else if (e.key === 'Enter') {
        e.preventDefault();
        toggleAutoPlay();
      }
    });
    
    // Demo button functions
    function watchSimulationDemo() {
      // Open simulation demo video in new tab or modal
      window.open('assets/videos/demos/full_simulation_demo.mp4', '_blank');
    }
    
    function watchRobotDemo() {
      // Open robot demo video in new tab or modal
      window.open('assets/videos/demos/full_robot_demo.mp4', '_blank');
    }
    
    // Navigation functionality
    document.addEventListener('DOMContentLoaded', function() {
      // Mobile menu toggle
      const navToggle = document.querySelector('.nav-toggle');
      const navMenu = document.querySelector('.nav-menu');
      
      if (navToggle && navMenu) {
        navToggle.addEventListener('click', function() {
          navMenu.classList.toggle('active');
          navToggle.classList.toggle('active');
        });
        
        // Close mobile menu when clicking on a link
        const navLinks = document.querySelectorAll('.nav-link');
        navLinks.forEach(link => {
          link.addEventListener('click', function() {
            navMenu.classList.remove('active');
            navToggle.classList.remove('active');
          });
        });
      }
      
      // Smooth scrolling for navigation links
      const navLinks = document.querySelectorAll('.nav-link[href^="#"]');
      navLinks.forEach(link => {
        link.addEventListener('click', function(e) {
          e.preventDefault();
          
          const targetId = this.getAttribute('href');
          const targetSection = document.querySelector(targetId);
          
          if (targetSection) {
            const offsetTop = targetSection.offsetTop - 80; // Account for fixed nav height
            window.scrollTo({
              top: offsetTop,
              behavior: 'smooth'
            });
          }
        });
      });
      
      // Active navigation highlighting based on scroll position
      function updateActiveNavLink() {
        const sections = document.querySelectorAll('section[id]');
        const navLinks = document.querySelectorAll('.nav-link');
        
        let currentSection = '';
        
        sections.forEach(section => {
          const sectionTop = section.offsetTop - 100;
          const sectionHeight = section.offsetHeight;
          
          if (window.scrollY >= sectionTop && window.scrollY < sectionTop + sectionHeight) {
            currentSection = section.getAttribute('id');
          }
        });
        
        navLinks.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === '#' + currentSection) {
            link.classList.add('active');
          }
        });
      }
      
      // Update active nav link on scroll
      window.addEventListener('scroll', updateActiveNavLink);
      
      // Initial call to set active nav link
      updateActiveNavLink();
    });

    // Interactive Architecture Functions
    function initializeInteractiveArchitecture() {
      const hotspots = document.querySelectorAll('.architecture-hotspot');
      const infoPanel = document.getElementById('info-panel-content');
      
      // Component information data
      const componentInfo = {
        // External Top Blocks - Long-Horizon Planning
        'long-horizon-planning': {
          title: 'Long-Horizon Planning',
          description: 'Involves high-level strategic planning for complex, long-term goals and multi-step tasks. Supports informed decision-making over extended time horizons by maintaining awareness of the environment and agent state. Breaks down large objectives into manageable sub-tasks and sequences that lower-level systems can execute. Adapts plans in response to unexpected events, ensuring resilience and flexibility. Learns new behaviors to handle unforeseen scenarios, extending capability beyond predefined conditions.'
        },
        
        // Metacognition
        'metacognition': {
          title: 'Metacognition',
          description: 'Supports self-monitoring and regulation of cognitive processes, allowing the system to reflect on its own thinking and decisions. Involves awareness of capabilities, limitations, and internal state for improved decision-making and resource use. Extends to understanding other agents’ states, intentions, and roles, enabling effective collaboration in multi-agent systems.'
        },
        
        // Explainability
        'explainability': {
          title: 'Explainability',
          description: 'Provides transparency into decision-making, enabling human understanding and trust. Supports inspection of logical steps with audit trails for debugging. Offers natural language explanations, allowing users to query and understand system behavior.'
        },
        
        // System 2: Strategic Layer
        'system2-layer': {
          title: 'System 2: Strategic (Cognitive) Layer',
          description: 'The strategic layer adapts the mature cognitive architecture OntoAgent for high-level reasoning, leveraging explicit, structured knowledge representations that can be inspected, verified, and incrementally expanded. This layer employs both utility-based and analogical reasoning, maintaining an ontological world model capable of supporting metacognition, episodic memory, and a situation model containing representations of current task context. It prioritizes goals, manages plan agendas, and selects actions while continuously monitoring their execution, facilitating team-oriented operations including natural language communication and explanation.'
        },
        'ontoagent-framework': {
          title: 'OntoAgent Framework',
          description: 'Built over a service-based ecosystem, OntoAgent includes processing modules for perception interpretation (separate module for each perception modality), attention management, goal and plan selection, and plan execution. Its knowledge substrate includes an ontological world model capable of supporting metacognition, knowledge support for interpretation of perception, an episodic memory of past events, and a situation model that contains representations of entities and events that are part of the current task context.'
        },
        
        // OntoAgent Components
        'perception-interpretation': {
          title: 'Perception Interpretation',
          description: 'The strategic layer interprets preprocessed multimodal perception data within the context of its situation model and active goals, generating normalized ontologically grounded text (TMR) and vision (VMR) meaning representations that formally specify the semantics of perceptual input. The TMRs and VMRs are added to OntoAgent\'s situation model, which provides data and heuristics for downstream functioning.'
        },
        'attention-service-s2': {
          title: 'Attention Service (Strategic)',
          description: 'Focuses cognitive resources on relevant information for strategic decision-making. Manages information filtering and prioritization at the cognitive level.'
        },
        'strategic-reasoning': {
          title: 'Strategic Reasoning & Decision-Making',
          description: 'Generates high-level plans and makes decisions based on long-term goals and knowledge. Handles complex reasoning about goals, constraints, and trade-offs.'
        },
        'rendering-services': {
          title: 'Rendering Services',
          description: 'When plan execution reaches a step that corresponds to an atomic action, a command is issued for the tactical layer to execute it. The strategic layer generates normalized ontologically grounded text (TMR) and vision (VMR) meaning representations that formally specify the semantics of perceptual input.'
        },
        
        // Communication APIs
        'perception-apis': {
          title: 'Perception Data Communication APIs',
          description: 'The tactical layer provides the strategic layer with preprocessed multimodal (speech, vision, etc.) perception data and relays robot state information, employing a suite of perception models within the perception services.'
        },
        'action-apis': {
          title: 'Action Data Communication APIs',
          description: 'The strategic and tactical layers are connected through a bidirectional interface that enables efficient inter-layer communication and data transfer. High-level action commands sent by the OntoAgent through the Interface APIs are unpacked and used to update corresponding variables in the blackboard.'
        },
        
        // System 1: Tactical Layer
        'system1-layer': {
          title: 'System 1: Tactical (Control) Layer',
          description: 'The tactical layer handles robotic control and execution, grounding cognitive capabilities in physical embodiment. It manages robot action planning, reflexive attention, and execution of motor actions in response to high-level commands from the strategic layer. The attention service in this layer interprets perceptual inputs and assigns priority to sensory signals before passing them to the attention service in the strategic layer.<br><br><div style="text-align: center; margin-top: 0px; width: 90%; margin-left: auto; margin-right: auto;"><video src="assets/videos/demos/tactical_animation.mp4" onclick="openTacticalVideoModal()" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1); cursor: pointer; transition: transform 0.2s ease;" onmouseover="this.style.transform=\'scale(1.02)\'" onmouseout="this.style.transform=\'scale(1)\'" controls muted loop autoplay></video><br><small style="color: var(--secondary-color); font-style: italic; margin-top: 0px; display: block;">Click on the video to zoom</small></div>'
        },
        
        // System 1 Components
        'perception-services': {
          title: 'Perception Services (Tactical)',
          description: 'Handles raw sensory input and provides real-time environmental awareness. Processes sensor data for immediate action and safety monitoring. Across the two systems, we use three different robots: a UGV and a Drone in simulation environments, and a tabletop serial manipulator. Each robot operates its own instance of OntoAgent at the strategic level and customized low-level planners at the tactical level.'
        },
        'attention-service-s1': {
          title: 'Attention Service (Tactical)',
          description: 'Directs sensory focus and processing for immediate task relevance. Manages real-time attention allocation for reactive behaviors. The tactical planners use a blackboard to keep track of condition and state variables, allowing for efficient querying and updating of the system\'s state during operation based on sensory inputs and action commands received from the strategic layer.'
        },
        'tactical-reasoning': {
          title: 'Tactical Reasoning & Decision-Making (Behavior Trees)',
          description: 'In the action translation pipeline of the tactical layer, specialized robotic planners, controllers, algorithms, and action sequence models are employed as part of the Reasoning and Decision Making modules and Effector Services to translate abstract action commands from OntoAgent into precise, executable robot control operations.'
        },
        'behavior-trees': {
          title: 'Behavior Trees',
          description: 'Implemented with Behavior Trees (BTs), the tactical layer both executes coordinated task sequences and reacts dynamically to the environment. The hierarchical BT structure supports real-time collision avoidance and adaptive behavior through task prioritization that can override or adjust plans as needed. BTs also enable selective use of specialized control policies, including whole-body compliance, motion planning, and path planning.<br><br><div style="text-align: center; margin-top: -20px; width: 75%; margin-left: auto; margin-right: auto;"><img src="assets/images/architecture/Robots-BTs.png" alt="Behavior Trees Structure" onclick="openBehaviorTreesModal()" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1); cursor: pointer; transition: transform 0.2s ease;" onmouseover="this.style.transform=\'scale(1.02)\'" onmouseout="this.style.transform=\'scale(1)\'"><br><small style="color: var(--secondary-color); font-style: italic; margin-top: 0px; display: block;">Click to zoom</small></div>'
        },
        'effector-services': {
          title: 'Effector Services',
          description: 'Translates tactical commands into physical actions for motors and actuators. Handles low-level control and safety constraints for physical execution. The integration infrastructure is developed in ROS2 for both robots and simulators, with simulation environments created in Unity. OntoAgent is developed in Python, incorporating various linguistic, machine learning, and statistical tools.'
        },
        
        // Perception Inputs
        'perception-inputs': {
          title: 'Perception Inputs',
          description: 'Various sensory data streams feeding into the system. Provides the raw information needed for environmental understanding and decision-making.'
        },
        'environment-perception': {
          title: 'Environment Perception',
          description: 'Visual and spatial data from the surrounding world. Includes camera feeds, depth sensors, and environmental mapping information.'
        },
        'speech-input': {
          title: 'Speech Input',
          description: 'Auditory data including human commands and communications. Processes voice commands and natural language interactions.'
        },
        'health-monitoring': {
          title: 'Health Monitoring',
          description: 'Internal system status and vital signs monitoring. Tracks battery levels, system health, and operational status indicators.'
        },
        
        // Action Outputs
        'action-outputs': {
          title: 'Action Outputs',
          description: 'Physical and communicative outputs generated by the system. Represents the system\'s ability to interact with and affect the environment.'
        },
        'motor-control': {
          title: 'Motor Control',
          description: 'Commands for controlling physical motors and movement systems. Handles locomotion, manipulation, and other motor-based actions.'
        },
        'actuator-control': {
          title: 'Actuator Control',
          description: 'Commands for various mechanical actuators including grippers, joints, and specialized tools. Enables precise manipulation and interaction.'
        },
        'audio-output': {
          title: 'Audio Output',
          description: 'Sound generation including speech synthesis, alerts, and audio feedback. Enables verbal communication and audio-based user interaction.'
        },
        
        // Bottom External Modules
        'perception-mapping': {
          title: 'Perception and Interpretation',
          description: 'Low-level processing for environmental sensing and map creation. Handles SLAM, object detection, and spatial understanding.'
        },
        'reactive-planning': {
          title: 'Reactive Planning (Safety and Needs)',
          description: 'Immediate, instinctual responses to ensure safety and fulfill basic needs. Provides emergency responses and basic survival behaviors.'
        },
        'short-horizon-plans': {
          title: 'Short Horizon Plans',
          description: 'Tactical plans for immediate future actions, often derived from strategic goals. Bridges the gap between high-level planning and immediate execution.'
        },
        'actuation-policies': {
          title: 'Actuation and Motor Control Policies',
          description: 'Specific rules and algorithms for controlling physical outputs. Implements safety constraints and control policies for physical actions.'
        }
      };
      
      hotspots.forEach(hotspot => {
        hotspot.addEventListener('click', function() {
          // Remove active class from all hotspots
          hotspots.forEach(h => h.classList.remove('active'));
          
          // Add active class to clicked hotspot
          this.classList.add('active');
          
          const component = this.getAttribute('data-component');
          const info = componentInfo[component];
          
          if (info && infoPanel) {
            // Determine color based on button class
            let textColor = '#333'; // default color
            if (this.classList.contains('dark-blue')) {
              textColor = '#1e3a8a';
            } else if (this.classList.contains('navy-blue')) {
              textColor = '#1e40af';
            } else if (this.classList.contains('black')) {
              textColor = '#000000';
            } else if (this.classList.contains('deep-green')) {
              textColor = '#065f46';
            } else if (this.classList.contains('dark-green')) {
              textColor = '#166534';
            }
            
            let content = `
              <h3 style="color: ${textColor};">${info.title}</h3>
              <p style="color: ${textColor};">${info.description}</p>
            `;
            
            // Add video if available
            if (info.video) {
              content += `
                <div class="video-container">
                  <video class="clickable-video" autoplay muted loop playsinline onclick="openVideoModal('${info.video.src}', '${info.video.caption}')">
                    <source src="${info.video.src}" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <div class="video-caption">${info.video.caption}</div>
                </div>
              `;
            }
            
            // Add image if available
            if (info.image) {
              content += `
                <div class="image-container" style="margin-top: 15px;">
                  <img src="${info.image.src}" alt="${info.image.alt}" class="clickable-image" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" onclick="openImageModal('${info.image.src}', '${info.image.alt}', '${info.image.caption}')">
                  <div class="image-caption" style="margin-top: 8px; font-size: 0.9em; color: #666; font-style: italic;">${info.image.caption}</div>
                </div>
              `;
            }
            
            infoPanel.innerHTML = content;
          }
        });
      });
      
      // Content persists when leaving the architecture area - no reset needed
    }

    // Lazy loading for videos
    document.addEventListener('DOMContentLoaded', function() {
      const videos = document.querySelectorAll('video.lazy-video');
      const observer = new IntersectionObserver(entries => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.play();
          } else {
            entry.target.pause();
          }
        });
      }, { threshold: 0.5 });
      
      videos.forEach(video => observer.observe(video));
      
      // Initialize demo
      initializeInteractiveDemo();
      
      // Initialize interactive architecture
      initializeInteractiveArchitecture();
      
      // Initialize image modal
      initializeImageModal();
      
      // Initialize video modal
      initializeVideoModal();
      
      // Initialize interactive demo 2
      initializeInteractiveDemo2();
    });
    
    // Image Modal Functions
    function openImageModal(src, alt, caption) {
      const modal = document.getElementById('imageModal');
      const modalImg = document.getElementById('modalImage');
      const modalCaption = document.getElementById('modalCaption');
      
      modal.style.display = 'block';
      modalImg.src = src;
      modalImg.alt = alt;
      modalCaption.innerHTML = caption;
      
      // Prevent body scroll when modal is open
      document.body.style.overflow = 'hidden';
    }
    
    function openBehaviorTreesModal() {
      const modal = document.getElementById('imageModal');
      const modalImg = document.getElementById('modalImage');
      const modalCaption = document.getElementById('modalCaption');
      
      modal.style.display = 'block';
      modalImg.src = 'assets/images/architecture/Robots-BTs.png';
      modalImg.alt = 'Behavior Trees Structure';
      modalCaption.innerHTML = '<strong>(a)</strong> UGV for ground-level exploration. <strong>(b)</strong> Parrot drone for aerial scanning. <strong>(c)</strong> 6-DoF robot. <strong>(d)</strong> Nodes in BTs. <strong>(e)</strong> BT design template for the robots\' tactical layer. <strong>(f)</strong> Sample BT on the UGV and 6-DoF robot.';
      
      // Prevent body scroll when modal is open
      document.body.style.overflow = 'hidden';
    }
    
    function openTacticalVideoModal() {
      const modal = document.getElementById('videoModal');
      const modalVideo = document.getElementById('modalVideo');
      const modalCaption = document.getElementById('modalVideoCaption');
      
      modal.style.display = 'block';
      modalVideo.src = 'assets/videos/demos/tactical_animation.mp4';
      modalCaption.textContent = 'Data-flow and control schematic illustrating the interactions between the behavior trees (BTs) in the tactical layer and the OntoAgent component in the strategic layer of the HARMONIC system.';
      
      // Start playing the video
      modalVideo.play();
      
      // Prevent body scroll when modal is open
      document.body.style.overflow = 'hidden';
    }
    
    function closeImageModal() {
      const modal = document.getElementById('imageModal');
      modal.style.display = 'none';
      
      // Restore body scroll
      document.body.style.overflow = 'auto';
    }
    
    function initializeImageModal() {
      const modal = document.getElementById('imageModal');
      const closeBtn = document.querySelector('.image-modal-close');
      
      // Close modal when clicking the X button
      closeBtn.addEventListener('click', closeImageModal);
      
      // Close modal when clicking outside the image
      modal.addEventListener('click', function(event) {
        if (event.target === modal) {
          closeImageModal();
        }
      });
      
      // Close modal when pressing Escape key
      document.addEventListener('keydown', function(event) {
        if (event.key === 'Escape' && modal.style.display === 'block') {
          closeImageModal();
        }
      });
    }
    
    // Video Modal Functions
    function openVideoModal(src, caption) {
      const modal = document.getElementById('videoModal');
      const modalVideo = document.getElementById('modalVideo');
      const modalCaption = document.getElementById('modalVideoCaption');
      
      modal.style.display = 'block';
      modalVideo.src = src;
      modalCaption.textContent = caption;
      
      // Start playing the video
      modalVideo.play();
      
      // Prevent body scroll when modal is open
      document.body.style.overflow = 'hidden';
    }
    
    function closeVideoModal() {
      const modal = document.getElementById('videoModal');
      const modalVideo = document.getElementById('modalVideo');
      
      modal.style.display = 'none';
      
      // Pause and reset the video
      modalVideo.pause();
      modalVideo.currentTime = 0;
      
      // Restore body scroll
      document.body.style.overflow = 'auto';
    }
    
    function initializeVideoModal() {
      const modal = document.getElementById('videoModal');
      const closeBtn = document.querySelector('.video-modal-close');
      
      // Close modal when clicking the X button
      closeBtn.addEventListener('click', closeVideoModal);
      
      // Close modal when clicking outside the video
      modal.addEventListener('click', function(event) {
        if (event.target === modal) {
          closeVideoModal();
        }
      });
      
      // Close modal when pressing Escape key
      document.addEventListener('keydown', function(event) {
        if (event.key === 'Escape' && modal.style.display === 'block') {
          closeVideoModal();
        }
      });
    }
    
    // Interactive Demo 2 Functions
    let currentStepIndex2 = 0;
    const totalSteps2 = 11;
    let autoPlayInterval2 = null;
    
    // Step titles for navigation
    const stepTitles2 = [
      'Understanding HARMONIC\'s Distributed Planning Process',
      'Task Initiation: Danny\'s Request',
      'Information Gathering: Object Description',
      'Location Information: Last Known Location',
      'Plan Distribution: Search Initiation',
      'Search Execution: Coordinated Exploration',
      'Communication and Coordination',
      'Object Detection: Keys Found',
      'Task Completion: Mission Report',
      'Mission Debrief: Team Coordination Summary',
      'Multi-Robot Demo Video'
    ];
    
    function initializeInteractiveDemo2() {
      generateStepNavigation2();
      updateStepDisplay2();
    }
    
    function generateStepNavigation2() {
      const stepNavGrid = document.getElementById('stepNavGrid2');
      if (!stepNavGrid) return;
      
      stepNavGrid.innerHTML = '';
      
      for (let i = 0; i < totalSteps2; i++) {
        const button = document.createElement('button');
        
        // Special styling for demo step
        if (i === 10) {
          button.className = 'step-nav-button simulation-nav-button';
          button.textContent = '10';
          button.title = `Step ${i}: ${stepTitles2[i]}`;
        } else {
          button.className = 'step-nav-button';
          button.textContent = i === 0 ? 'Intro' : i;
          button.title = `Step ${i}: ${stepTitles2[i]}`;
        }
        
        button.onclick = () => goToStep2(i);
        stepNavGrid.appendChild(button);
      }
    }
    
    function updateStepNavigation2() {
      const navButtons = document.querySelectorAll('#stepNavGrid2 .step-nav-button');
      navButtons.forEach((button, index) => {
        const stepNumber = index;
        button.classList.remove('active', 'completed');
        
        if (stepNumber === currentStepIndex2) {
          button.classList.add('active');
        } else if (stepNumber < currentStepIndex2) {
          button.classList.add('completed');
        }
      });
    }
    
    function updateStepDisplay2(shouldScroll = false) {
      // Update step counter
      document.getElementById('currentStep2').textContent = currentStepIndex2;
      document.getElementById('totalSteps2').textContent = totalSteps2 - 1;
      
      // Update button states
      document.getElementById('prevStep2').disabled = currentStepIndex2 <= 0;
      document.getElementById('nextStep2').disabled = currentStepIndex2 >= totalSteps2 - 1;
      
      // Update step navigation buttons
      updateStepNavigation2();
      
      // Hide all steps
      const steps = document.querySelectorAll('#demoContent2 .demo-step');
      steps.forEach(step => {
        step.classList.remove('active');
        step.style.display = 'none';
      });
      
      // Show current step
      const currentStep = document.getElementById(`step2-${currentStepIndex2}`);
      if (currentStep) {
        currentStep.classList.add('active');
        currentStep.style.display = 'block';
        
        if (shouldScroll) {
          currentStep.scrollIntoView({ behavior: 'smooth', block: 'start' });
        }
      }
    }
    
    function goToStep2(stepIndex) {
      if (stepIndex >= 0 && stepIndex < totalSteps2) {
        currentStepIndex2 = stepIndex;
        updateStepDisplay2(false);
      }
    }
    
    function nextStep2() {
      if (currentStepIndex2 < totalSteps2 - 1) {
        currentStepIndex2++;
        updateStepDisplay2(false);
      }
    }
    
    function previousStep2() {
      if (currentStepIndex2 > 0) {
        currentStepIndex2--;
        updateStepDisplay2(false);
      }
    }
    
    function toggleAutoPlay2() {
      const button = document.getElementById('autoPlayBtn2');
      
      if (autoPlayInterval2) {
        clearInterval(autoPlayInterval2);
        autoPlayInterval2 = null;
        button.textContent = 'Auto Play';
        button.style.background = '';
      } else {
        autoPlayInterval2 = setInterval(() => {
          if (currentStepIndex2 < totalSteps2 - 1) {
            nextStep2();
          } else {
            toggleAutoPlay2(); // Stop auto-play at the end
          }
        }, 3000); // 3 seconds per step
        button.textContent = 'Stop Auto Play';
        button.style.background = 'linear-gradient(135deg, #e74c3c 0%, #c0392b 100%)';
      }
    }
  </script>
  
  <!-- Image Modal -->
  <div id="imageModal" class="image-modal">
    <div class="image-modal-content">
      <span class="image-modal-close">&times;</span>
      <img id="modalImage" src="" alt="">
      <div id="modalCaption" class="image-modal-caption"></div>
    </div>
  </div>
  
  <!-- Video Modal -->
  <div id="videoModal" class="video-modal">
    <div class="video-modal-content">
      <span class="video-modal-close">&times;</span>
      <video id="modalVideo" controls autoplay muted loop playsinline>
        <source src="" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <div id="modalVideoCaption" class="video-modal-caption"></div>
    </div>
  </div>
</body>
</html>